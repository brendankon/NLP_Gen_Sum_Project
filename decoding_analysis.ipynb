{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 162,
     "status": "ok",
     "timestamp": 1666530433739,
     "user": {
      "displayName": "Brendan Kondracki",
      "userId": "14824001390510818675"
     },
     "user_tz": 240
    },
    "id": "Q5aKyIH0wAiO",
    "outputId": "992681e6-0a10-4fd2-e901-a51f78278cac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 19 00:21:05 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   70C    P0    33W /  70W |      0MiB / 15360MiB |     10%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11674,
     "status": "ok",
     "timestamp": 1666732281144,
     "user": {
      "displayName": "Brendan Kondracki",
      "userId": "14824001390510818675"
     },
     "user_tz": 240
    },
    "id": "aANOi-AKaaTE"
   },
   "outputs": [],
   "source": [
    "#Dataset class for CodeT5 APPS training data\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from torch.utils import data\n",
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "class APPSDataset(data.Dataset):\n",
    "    \"\"\"APPS dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data, tokenizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        expanded_data = []\n",
    "        for sample in data:\n",
    "            if sample[\"starter_code\"] == '':\n",
    "                input_string = \"\\nQUESTION:\\n\" + sample[\"question\"] + \"\\n\" + sample[\"starter_code\"] + \"\\n\" + \"\\nUse Standard Input Format\\n\\nANSWER:\\n\"\n",
    "            else:\n",
    "                input_string = \"\\nQUESTION:\\n\" + sample[\"question\"] + \"\\n\" + sample[\"starter_code\"] + \"\\n\" + \"\\nUse Call-Based Format\\n\\nANSWER:\\n\"\n",
    "            ids_object = self.tokenizer(input_string, return_tensors=\"pt\")\n",
    "            input_ids = ids_object.input_ids[0]\n",
    "            if len(input_ids) > 512:\n",
    "                continue\n",
    "            if len(sample[\"solutions\"]) > 0:\n",
    "                sample[\"solutions\"] = json.loads(sample[\"solutions\"])\n",
    "            for solution in sample[\"solutions\"]:\n",
    "                ids_object = self.tokenizer(solution, return_tensors=\"pt\")\n",
    "                input_ids = ids_object.input_ids[0]\n",
    "                if len(input_ids) > 512:\n",
    "                    continue\n",
    "                expanded_data.append({\"model_input\": input_string, \"question\": sample[\"question\"], \"solution\": solution, \"starter_code\": sample[\"starter_code\"], \"input_output\": sample[\"input_output\"], \"difficulty\": sample[\"difficulty\"]})\n",
    "        self.data = expanded_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.numpy()\n",
    "\n",
    "        ids_object = self.tokenizer(self.data[idx][\"model_input\"], return_tensors=\"pt\", padding='max_length')\n",
    "        input_ids = ids_object.input_ids[0]\n",
    "        attention_mask = ids_object.attention_mask[0]\n",
    "\n",
    "        output_ids_object = self.tokenizer(self.data[idx][\"solution\"], return_tensors=\"pt\", padding='max_length')\n",
    "        output_ids = output_ids_object.input_ids[0]\n",
    "        output_attention_mask = output_ids_object.attention_mask[0]\n",
    "\n",
    "        sample = {\"source_ids\": input_ids, \"source_mask\": attention_mask, \"target_ids\": output_ids, \"target_mask\": output_attention_mask}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1666732281145,
     "user": {
      "displayName": "Brendan Kondracki",
      "userId": "14824001390510818675"
     },
     "user_tz": 240
    },
    "id": "60R5Mga3YLeB"
   },
   "outputs": [],
   "source": [
    "#Dataset class for CodeT5 APPS test data\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from torch.utils import data\n",
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "class APPSTestDataset(data.Dataset):\n",
    "    \"\"\"APPS dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data, tokenizer, problem_type):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        expanded_data = []\n",
    "        for sample in data:\n",
    "            if sample[\"starter_code\"] == '':\n",
    "                input_string = \"\\nQUESTION:\\n\" + sample[\"question\"] + \"\\n\" + sample[\"starter_code\"] + \"\\n\" + \"\\nUse Standard Input Format\\n\\nANSWER:\\n\"\n",
    "            else:\n",
    "                input_string = \"\\nQUESTION:\\n\" + sample[\"question\"] + \"\\n\" + sample[\"starter_code\"] + \"\\n\" + \"\\nUse Call-Based Format\\n\\nANSWER:\\n\"\n",
    "            ids_object = self.tokenizer(input_string, return_tensors=\"pt\")\n",
    "            input_ids = ids_object.input_ids[0]\n",
    "            if len(input_ids) > 512 or sample[\"difficulty\"] != problem_type:\n",
    "                continue\n",
    "            expanded_data.append({\"problem_id\": sample[\"problem_id\"], \"model_input\": input_string, \"question\": sample[\"question\"], \"starter_code\": sample[\"starter_code\"], \"difficulty\": sample[\"difficulty\"]})\n",
    "        self.data = expanded_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.numpy()\n",
    "\n",
    "        ids_object = self.tokenizer(self.data[idx][\"model_input\"], return_tensors=\"pt\", padding='max_length')\n",
    "        input_ids = ids_object.input_ids[0]\n",
    "        attention_mask = ids_object.attention_mask[0]\n",
    "\n",
    "        sample = {\"source_ids\": input_ids, \"source_mask\": attention_mask}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset class for GPT-2 APPS test data\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from torch.utils import data\n",
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "class GPTAPPSTestDataset(data.Dataset):\n",
    "    \"\"\"APPS dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data, tokenizer, gpt_tokenizer, problem_type):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        expanded_data = []\n",
    "        for sample in data:\n",
    "            if sample[\"starter_code\"] == '':\n",
    "                input_string = \"\\nQUESTION:\\n\" + sample[\"question\"] + \"\\n\" + sample[\"starter_code\"] + \"\\n\" + \"\\nUse Standard Input Format\\n\\nANSWER:\\n\"\n",
    "            else:\n",
    "                input_string = \"\\nQUESTION:\\n\" + sample[\"question\"] + \"\\n\" + sample[\"starter_code\"] + \"\\n\" + \"\\nUse Call-Based Format\\n\\nANSWER:\\n\"\n",
    "            ids_object = self.tokenizer(input_string, return_tensors=\"pt\")\n",
    "            input_ids = ids_object.input_ids[0]\n",
    "            if len(input_ids) > 512 or sample[\"difficulty\"] != problem_type:\n",
    "                continue\n",
    "            expanded_data.append({\"problem_id\": sample[\"problem_id\"], \"model_input\": input_string, \"question\": sample[\"question\"], \"starter_code\": sample[\"starter_code\"], \"difficulty\": sample[\"difficulty\"]})\n",
    "        self.data = expanded_data\n",
    "        self.tokenizer = gpt_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.numpy()\n",
    "\n",
    "        input_ids = torch.LongTensor(self.tokenizer.encode(self.data[idx][\"model_input\"], verbose=False)).unsqueeze(0)\n",
    "\n",
    "        sample = {\"source_ids\": input_ids}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364,
     "referenced_widgets": [
      "7ecce60e38414b5e816f0a73d348980d",
      "5779574a9dbe4a309fffa69d2a0fecea",
      "56f30b99e3a2416b8f4aa25422d846b2",
      "38922a79fbe2414589d76f73a79a0635",
      "7f370a102aa94d3e9f518910fe73deac",
      "0cff02e713b7475f90e86a511a13e27b",
      "070ed550a6ba4a7aaaeb3ba15c4066a0",
      "efd59de2ec8f486eab54d39b14835b27",
      "3ce4e85230eb4f22bdd1214fb963182b",
      "19fbd956215a4b8987f1fab9e1cc4a75",
      "d4cc299e543145e49c51c5a5a1194326",
      "067106e33595486282ab1c67a878093e",
      "342b2d59758743d18e8b7be77f47d2cf",
      "a78807ba67b34e2b8f306e49603e895b",
      "28ea636469a74af2916baa976379c761",
      "a7c3e7b8ade14c41b77f9d3bd357dddb",
      "9951679739c94f58bcad9a07974371df",
      "181442a9ff0547d28f4aa33423b8c1b2",
      "f350d756c0354010b4f5d45ec779d9cc",
      "ec70235fc1d74a2499dad4db00296ef8",
      "731caaf300ba4da8a02da6951596fd99",
      "8c0e3e14eee044dc9bbf5fcfe8a6b853",
      "969ea53b2bbe4e9db098c413dae1a0e6",
      "75516b3bbcd64743ba028bab31576813",
      "ab76d1693f9641079c0d0ee19b7aa096",
      "689fe2225f614aef9fd7e3786fc63d85",
      "67374604e7264f6abdf54c8edc062842",
      "75c2dbc8dbb54620b1008e43f28d8855",
      "d78b1cedb1524233b8c803e1bb8b64f9",
      "b220d2c5753349a39eaca4e2f341fd6b",
      "31c848ab8292448bb6b7f3689a01ae42",
      "0eff06df5d6641ee8ac379e31ac59fda",
      "4f5119ceba7d4893a216c04727c162d7",
      "72e6a1befe094dcca90ba6a331b2fb7c",
      "1beab5cdb547417aaa9f000456318095",
      "8f20595f01ae43cab4267e6464aa9772",
      "c1c0ef5a62a24f9cb0856c333d45f9c0",
      "95ef1c79b7734a65826842bd8771bf30",
      "227027e946804af082abb9b90053314e",
      "b5d96b10092e4377b95d072e56f16ac5",
      "7a01aa7f65cb41eb82f1082484873bec",
      "17aeb3f915f14cbb80287500a1962a29",
      "8d88b9208d4a4cf9bee8328b634ae9dd",
      "4f16305129c845f7a52c40be80b8998b",
      "b3fd6337545a4f40b38a6c0002071576",
      "87108eee975446f6843700cb9c1e7260",
      "389b7172c6c34870b38e4cfe7e8267cd",
      "4d6a202047344cf68643ead0dd62125c",
      "e6eb0bcfba554f0face54edda32bf55d",
      "569b94e34cab4a59acfb5cc18c82a4d5",
      "5976191eec824afd8a28394e3b54a2fa",
      "d52ab756e4014fbd813cc236e1d80a8e",
      "e09fe2cefb4449dcbc3f329fe8531682",
      "580d26863a6045ba88e5faa303426551",
      "04ae4ba4bb3e4f1c9d181303a74dd901",
      "8bb57d4a1fca4304a0880f59e304f957",
      "a4bd17fcb67d4504a520b5ab89a3cf7a",
      "df4030a26afa48cf98cea1eef8c1719e",
      "0dc81039a8a44c92b01c620b318661f4",
      "8ca42afeb33c48d88d4fe6e91b02a453",
      "28c1d8c47b0d41a7aea4e4eeb711daa7",
      "ce50f086f2c74661a44589422022b38d",
      "63f21c09f0c142b78881d58de21da94e",
      "6e5962ddd1ad4fe58cafb364e02f05ea",
      "62eba812c1ea420bbba424761ae04512",
      "4cefa04beb45437b82585dce4f2dc07a",
      "4e204ed303614cae8900afe9ac6a8ff9",
      "3057b9dfcdc54b8d85df164743b2d816",
      "60d57f563fc34d43a7f247535591915c",
      "cc6c5c0f01d4488e893ed17549a5c352",
      "ca33183589f24d3eb88e8a8768bd3793",
      "42b645c7747e4ab3b14a7132174d62c2",
      "2678115b6c0543a48db475aa836ad3d0",
      "92aa058fc276497182afd25da53586ee",
      "09d7f2349f034892999122bc3f163d39",
      "bdceb0ea847a46ad8e2a4eb0bff2ff47",
      "02b89793ece04cb882adc21c6b5017df",
      "287849c76dd54795a248258bc97418eb",
      "23814bd71b2b41efb2cb30f0d9026d1d",
      "d4d35361775c4f08af96044d6641e3cd",
      "96de6db31cb34810bab503906d37ffa3",
      "95b19c1008d9476d90ce2428ec830cbd",
      "4ae819c352f1436e8a8edbf070b614a1",
      "517071c32d1540f987393c62c9ae7fda",
      "6350069db64b4de9a20bfba57cb307b7",
      "41dfba0fc6634e3e87744e16b168fcd4",
      "91edd4419ffa435cab242d6f6a1bd4ae",
      "7f37259c65c24896aee34c6ef91b56d2",
      "7ec0d2e7beeb453ca105e937e25e4649",
      "21068c7def7f430aa7dde408bb4bbcf8",
      "db77ee59a9a04054b905ad6029194f40",
      "5c7454324a0f435cb02740e1a7c329a1",
      "e765fb6b14a841ffae367ea9a1c10673",
      "365d539c8d904e84869d87cfe4826776",
      "8ff46e04f6ab4ed2869bc3895269d9a0",
      "eded64c3dfae4bc39806e54dcdaecb1d",
      "277ebcdd32df4b8db9085d9594e2fae5",
      "fd00f5d4b930408fa6906459c550c4a9",
      "7e6de5e403534d0097ab835d4efec819",
      "f9619fca2b574be281e94028d9be8fc4",
      "60ca3e674bb5446e90fb8800190ceab0",
      "ff3a1a0b7cea449f9843b998a56ec3b2",
      "12535537221449a08b56ffad4da68b74",
      "e1715240dd784cc684cbf7c6839e43d8",
      "c9dfe42049d0417699ad332110998f38",
      "7adae587a4f542adacca30641df7ffb8",
      "658b1810ed534be696ce167905293691",
      "a1d0d228170247e2a62c1dd490114481",
      "b20913079b8d4e6f8acc14a97e4c1302",
      "13fca227fa7b455aa9c015fa10e99731"
     ]
    },
    "executionInfo": {
     "elapsed": 282353,
     "status": "ok",
     "timestamp": 1666732614444,
     "user": {
      "displayName": "Brendan Kondracki",
      "userId": "14824001390510818675"
     },
     "user_tz": 240
    },
    "id": "i8GXb62FdF0f",
    "outputId": "1c22d324-f4e8-45c5-cc64-41b35a8f6c4d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"codeparrot/apps\", split=\"train\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "train_data = APPSDataset(ds, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25688,
     "status": "ok",
     "timestamp": 1666732640124,
     "user": {
      "displayName": "Brendan Kondracki",
      "userId": "14824001390510818675"
     },
     "user_tz": 240
    },
    "id": "nIlSbfaSY4DQ",
    "outputId": "e3c16a9a-2c05-4dd0-8b0c-774b6226d5da"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "test_ds = load_dataset(\"codeparrot/apps\", split=\"test\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "test_data = APPSTestDataset(test_ds, tokenizer, \"interview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "gpt_test_ds = load_dataset(\"codeparrot/apps\", split=\"test\")\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_test_data = GPTAPPSTestDataset(gpt_test_ds, tokenizer, gpt_tokenizer, \"interview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 160,
     "status": "ok",
     "timestamp": 1666454182407,
     "user": {
      "displayName": "Brendan Kondracki",
      "userId": "14824001390510818675"
     },
     "user_tz": 240
    },
    "id": "ZKzfe8ciZDci",
    "outputId": "6c710447-a71c-4472-9f64-37777c254de1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1294"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "664"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gpt_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112,
     "status": "ok",
     "timestamp": 1665841733712,
     "user": {
      "displayName": "Brendan Kondracki",
      "userId": "14824001390510818675"
     },
     "user_tz": 240
    },
    "id": "Nyi7IyGKsxa9",
    "outputId": "e3604e50-42a4-42fa-af2d-c0bcc6b74ab9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86338"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5 fine tuner implementation utilized in part from: https://www.kaggle.com/code/parthplc/t5-fine-tuning-tutorial/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 951,
     "status": "ok",
     "timestamp": 1666732647798,
     "user": {
      "displayName": "Brendan Kondracki",
      "userId": "14824001390510818675"
     },
     "user_tz": 240
    },
    "id": "m0RhgiQ0t5nD"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "class T5FineTuner(pl.LightningModule):\n",
    "  def __init__(self, hparams, train_dataset, model):\n",
    "    super(T5FineTuner, self).__init__()\n",
    "    self.hparams.update(vars(hparams))\n",
    "    self.train_dataset = train_dataset\n",
    "    self.model = model\n",
    "    self.tokenizer = RobertaTokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n",
    "  \n",
    "  def is_logger(self):\n",
    "    return True\n",
    "  \n",
    "  def forward(\n",
    "      self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, labels=None\n",
    "  ):\n",
    "    return self.model(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        decoder_attention_mask=decoder_attention_mask,\n",
    "        labels=labels,\n",
    "    )\n",
    "\n",
    "  def _step(self, batch):\n",
    "    labels = batch[\"target_ids\"]\n",
    "    labels[labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "    outputs = self(\n",
    "        input_ids=batch[\"source_ids\"],\n",
    "        attention_mask=batch[\"source_mask\"],\n",
    "        labels=labels,\n",
    "        decoder_attention_mask=batch['target_mask']\n",
    "    )\n",
    "\n",
    "    loss = outputs[0]\n",
    "\n",
    "    return loss\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    loss = self._step(batch)\n",
    "\n",
    "    tensorboard_logs = {\"train_loss\": loss}\n",
    "    return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "  \n",
    "  def training_epoch_end(self, outputs):\n",
    "    avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "    #tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
    "    self.log(\"avg_train_loss\", avg_train_loss)\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    loss = self._step(batch)\n",
    "    return {\"val_loss\": loss}\n",
    "  \n",
    "  def validation_epoch_end(self, outputs):\n",
    "    avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "    tensorboard_logs = {\"val_loss\": avg_loss}\n",
    "    return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "\n",
    "    model = self.model\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": self.hparams.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "    self.opt = optimizer\n",
    "    return [optimizer]\n",
    "  \n",
    "  def optimizer_step(self,\n",
    "                     epoch=None, \n",
    "                    batch_idx=None, \n",
    "                    optimizer=None, \n",
    "                    optimizer_idx=None, \n",
    "                    optimizer_closure=None, \n",
    "                    on_tpu=None, \n",
    "                    using_native_amp=None, \n",
    "                    using_lbfgs=None\n",
    "                     ):\n",
    "\n",
    "    optimizer.step(closure=optimizer_closure)\n",
    "    optimizer.zero_grad()\n",
    "    self.lr_scheduler.step()\n",
    "  \n",
    "  def get_tqdm_dict(self):\n",
    "    tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
    "\n",
    "    return tqdm_dict\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    dataloader = data.DataLoader(self.train_dataset, batch_size=self.hparams.train_batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    t_total = (\n",
    "        (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
    "        // self.hparams.gradient_accumulation_steps\n",
    "        * float(self.hparams.num_train_epochs)\n",
    "    )\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "    self.lr_scheduler = scheduler\n",
    "    return dataloader\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    return data.DataLoader(self.train_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1666732647975,
     "user": {
      "displayName": "Brendan Kondracki",
      "userId": "14824001390510818675"
     },
     "user_tz": 240
    },
    "id": "owtBFLJYt9se"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LoggingCallback(pl.Callback):\n",
    "  def on_validation_end(self, trainer, pl_module):\n",
    "    logger.info(\"***** Validation results *****\")\n",
    "    if pl_module.is_logger():\n",
    "      metrics = trainer.callback_metrics\n",
    "      # Log results\n",
    "      for key in sorted(metrics):\n",
    "        if key not in [\"log\", \"progress_bar\"]:\n",
    "          logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "\n",
    "  def on_test_end(self, trainer, pl_module):\n",
    "    logger.info(\"***** Test results *****\")\n",
    "\n",
    "    if pl_module.is_logger():\n",
    "      metrics = trainer.callback_metrics\n",
    "\n",
    "      # Log and save results to file\n",
    "      output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n",
    "      with open(output_test_results_file, \"w\") as writer:\n",
    "        for key in sorted(metrics):\n",
    "          if key not in [\"log\", \"progress_bar\"]:\n",
    "            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1666732647975,
     "user": {
      "displayName": "Brendan Kondracki",
      "userId": "14824001390510818675"
     },
     "user_tz": 240
    },
    "id": "zdvQ8FxouBKn"
   },
   "outputs": [],
   "source": [
    "args_dict = dict(\n",
    "    data_dir=\"\",\n",
    "    output_dir=\"./NLP_Gen_Sum_Project\",\n",
    "    model_name_or_path='Salesforce/codet5-base',\n",
    "    tokenizer_name_or_path='Salesforce/codet5-base',\n",
    "    max_seq_length=512,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.0,\n",
    "    adam_epsilon=1e-8,\n",
    "    warmup_steps=0,\n",
    "    train_batch_size=8,\n",
    "    eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    gradient_accumulation_steps=32,\n",
    "    n_gpu=1,\n",
    "    fp_16=False,\n",
    "    max_grad_norm=1.0,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1666732647976,
     "user": {
      "displayName": "Brendan Kondracki",
      "userId": "14824001390510818675"
     },
     "user_tz": 240
    },
    "id": "-T334Gi6uPkQ"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "args = argparse.Namespace(**args_dict)\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=args.output_dir, monitor=\"avg_train_loss\", mode=\"min\", save_top_k=5\n",
    ")\n",
    "\n",
    "train_params = dict(\n",
    "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "    gpus=args.n_gpu,\n",
    "    max_epochs=args.num_train_epochs,\n",
    "    precision= 16 if args.fp_16 else 32,\n",
    "    gradient_clip_val=args.max_grad_norm,\n",
    "    callbacks=[LoggingCallback(), checkpoint_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1666732324284,
     "user": {
      "displayName": "Brendan Kondracki",
      "userId": "14824001390510818675"
     },
     "user_tz": 240
    },
    "id": "znzKgzqfuoXv"
   },
   "outputs": [],
   "source": [
    "#Train CodeT5 model and save it for future use\n",
    "model_base = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\n",
    "trainer = pl.Trainer(**train_params)\n",
    "model = T5FineTuner(args, train_data, model_base)\n",
    "model.load_state_dict(torch.load(\"./NLP_Gen_Sum_Project/Models/model_codet5_base_APPS.pt\"))\n",
    "trainer.fit(model)\n",
    "torch.save(model.state_dict(), \"./NLP_Gen_Sum_Project/model_codet5_base_APPS.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99,
     "referenced_widgets": [
      "4989973854db404d9778dc255dda8b5f",
      "f01aaaf2349243f49ed30165209c226f",
      "725cf25b463e4f96909a9ae4c119ce3a",
      "0c316286f2234849a2ea62780ecb72bc",
      "15a85533473a441fb198aa9b6d8f1109",
      "7e78c450df2547dbba98e905bf9e51fe",
      "d13e0fd435b74c3cbcacf8d7518a3c05",
      "b29352f173934596a549082fc978a9b7",
      "02dbb05070de4fb99038f3c1459d971b",
      "840ef064a4834daaa8a36ef53b0d6fff",
      "1de1621ea92f4ad6ace210c58ec50f6a",
      "6b90aaef56614dca8337d0049908ccaf",
      "d8c0a29c8e444d5f83a2d2db6a00f70e",
      "c0c5f585eed542fd86989f87d3230f87",
      "7ac9b67cd2e74ec680d3682e26c7d62e",
      "f432e1323022494682c1a642d5ae97d4",
      "1499dca31374425e90ca96e0a8f0eb4c",
      "d8b0cab1a00c4ada8a354355b94028b1",
      "b13708015b48461d904803cb3ddaf898",
      "46b29ecc681b4f4bb50d53139eb71ed5",
      "d3c49e2dfa2f4b6db7f267758e90ed18",
      "0f3887f816b94e22bb8a6af06ec4c0af"
     ]
    },
    "executionInfo": {
     "elapsed": 55656,
     "status": "ok",
     "timestamp": 1666732703616,
     "user": {
      "displayName": "Brendan Kondracki",
      "userId": "14824001390510818675"
     },
     "user_tz": 240
    },
    "id": "Rdva_Ie4zoVM",
    "outputId": "1c434f5b-ee14-4c7c-f3a0-16b20e3b3e22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_base = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\n",
    "model = T5FineTuner(args, test_data, model_base)\n",
    "model.load_state_dict(torch.load(\"model_codet5_base_APPS.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1666732703616,
     "user": {
      "displayName": "Brendan Kondracki",
      "userId": "14824001390510818675"
     },
     "user_tz": 240
    },
    "id": "mBP5I5CS0TNH",
    "outputId": "1631f2ce-9d2a-4397-a619-ff3723d8c70a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START: COPIED FROM https://github.com/huggingface/transformers/blob/v4.25.1/src/transformers/generation/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 480,
     "status": "ok",
     "timestamp": 1666732704066,
     "user": {
      "displayName": "Brendan Kondracki",
      "userId": "14824001390510818675"
     },
     "user_tz": 240
    },
    "id": "2x_WgP3NaNTF"
   },
   "outputs": [],
   "source": [
    "from transformers import BeamScorer\n",
    "from typing import List, Optional, Tuple\n",
    "from collections import UserDict\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "class CodeBeamSearchScorer(BeamScorer):\n",
    "    r\"\"\"\n",
    "    [`BeamScorer`] implementing standard beam search decoding.\n",
    "    Adapted in part from [Facebook's XLM beam search\n",
    "    code](https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529).\n",
    "    Reference for the diverse beam search algorithm and implementation [Ashwin Kalyan's DBS\n",
    "    implementation](https://github.com/ashwinkalyan/dbs/blob/master/dbs/beam_utils.lua)\n",
    "    Args:\n",
    "        batch_size (`int`):\n",
    "            Batch Size of `input_ids` for which standard beam search decoding is run in parallel.\n",
    "        max_length (`int`):\n",
    "            The maximum length of the sequence to be generated.\n",
    "        num_beams (`int`):\n",
    "            Number of beams for beam search.\n",
    "        device (`torch.device`):\n",
    "            Defines the device type (*e.g.*, `\"cpu\"` or `\"cuda\"`) on which this instance of `BeamSearchScorer` will be\n",
    "            allocated.\n",
    "        length_penalty (`float`, *optional*, defaults to 1.0):\n",
    "            Exponential penalty to the length that is used with beam-based generation. It is applied as an exponent to\n",
    "            the sequence length, which in turn is used to divide the score of the sequence. Since the score is the log\n",
    "            likelihood of the sequence (i.e. negative), `length_penalty` > 0.0 promotes longer sequences, while\n",
    "            `length_penalty` < 0.0 encourages shorter sequences.\n",
    "        do_early_stopping (`bool`, *optional*, defaults to `False`):\n",
    "            Whether to stop the beam search when at least `num_beams` sentences are finished per batch or not.\n",
    "        num_beam_hyps_to_keep (`int`, *optional*, defaults to 1):\n",
    "            The number of beam hypotheses that shall be returned upon calling\n",
    "            [`~transformer.BeamSearchScorer.finalize`].\n",
    "        num_beam_groups (`int`):\n",
    "            Number of groups to divide `num_beams` into in order to ensure diversity among different groups of beams.\n",
    "            See [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.\n",
    "    \"\"\"\n",
    "    @torch.no_grad()\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        num_beams: int,\n",
    "        device: torch.device,\n",
    "        length_penalty: Optional[float] = 1.0,\n",
    "        lambd: Optional[float] = 0,\n",
    "        gamma: Optional[float] = 0,\n",
    "        do_early_stopping: Optional[bool] = False,\n",
    "        num_beam_hyps_to_keep: Optional[int] = 1,\n",
    "        num_beam_groups: Optional[int] = 1,\n",
    "        new_line_token: Optional[int] = 0,\n",
    "        loop_tokens: Optional[list] = [],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.num_beams = num_beams\n",
    "        self.device = device\n",
    "        self.length_penalty = length_penalty\n",
    "        self.lambd = lambd\n",
    "        self.gamma = gamma\n",
    "        self.do_early_stopping = do_early_stopping\n",
    "        self.num_beam_hyps_to_keep = num_beam_hyps_to_keep\n",
    "        self.num_beam_groups = num_beam_groups\n",
    "        self.group_size = self.num_beams // self.num_beam_groups\n",
    "        self.new_line_token = new_line_token\n",
    "        self.loop_tokens = loop_tokens\n",
    "\n",
    "        self._is_init = False\n",
    "        self._beam_hyps = [\n",
    "            BeamHypotheses(\n",
    "                num_beams=self.num_beams,\n",
    "                length_penalty=self.length_penalty,\n",
    "                lambd = self.lambd,\n",
    "                gamma = self.gamma,\n",
    "                early_stopping=self.do_early_stopping,\n",
    "                new_line_token = self.new_line_token,\n",
    "                loop_tokens = self.loop_tokens\n",
    "            )\n",
    "            for _ in range(batch_size)\n",
    "        ]\n",
    "        self._done = torch.tensor([False for _ in range(batch_size)], dtype=torch.bool, device=self.device)\n",
    "\n",
    "        if not isinstance(num_beams, int) or num_beams <= 1:\n",
    "            raise ValueError(\n",
    "                f\"`num_beams` has to be an integer strictly greater than 1, but is {num_beams}. For `num_beams` == 1,\"\n",
    "                \" one should make use of `greedy_search` instead.\"\n",
    "            )\n",
    "\n",
    "        if not isinstance(num_beam_groups, int) or (num_beam_groups > num_beams) or (num_beams % num_beam_groups != 0):\n",
    "            raise ValueError(\n",
    "                \"`num_beam_groups` has to be an integer smaller or equal than `num_beams` and `num_beams` has to be\"\n",
    "                f\" divisible by `num_beam_groups`, but is {num_beam_groups} with `num_beams` being {num_beams}.\"\n",
    "            )\n",
    "\n",
    "        if \"max_length\" in kwargs:\n",
    "            warnings.warn(\n",
    "                \"Passing `max_length` to BeamSearchScorer is deprecated and has no effect. \"\n",
    "                \"`max_length` should be passed directly to `beam_search(...)`, `beam_sample(...)`\"\n",
    "                \", or `group_beam_search(...)`.\"\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def is_done(self) -> bool:\n",
    "        return self._done.all()\n",
    "    @torch.no_grad()\n",
    "    def process(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        next_scores: torch.FloatTensor,\n",
    "        next_tokens: torch.LongTensor,\n",
    "        next_indices: torch.LongTensor,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[int] = None,\n",
    "        beam_indices: Optional[torch.LongTensor] = None,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        cur_len = input_ids.shape[-1]\n",
    "        batch_size = len(self._beam_hyps)\n",
    "        if not (batch_size == (input_ids.shape[0] // self.group_size)):\n",
    "            if self.num_beam_groups > 1:\n",
    "                raise ValueError(\n",
    "                    f\"A group beam size of {input_ids.shape[0]} is used as the input, but a group beam \"\n",
    "                    f\"size of {self.group_size} is expected by the beam scorer.\"\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"A beam size of {input_ids.shape[0]} is used as the input, but a beam size of \"\n",
    "                    f\"{self.group_size} is expected by the beam scorer.\"\n",
    "                )\n",
    "\n",
    "        device = input_ids.device\n",
    "        next_beam_scores = torch.zeros((batch_size, self.group_size), dtype=next_scores.dtype, device=device)\n",
    "        next_beam_tokens = torch.zeros((batch_size, self.group_size), dtype=next_tokens.dtype, device=device)\n",
    "        next_beam_indices = torch.zeros((batch_size, self.group_size), dtype=next_indices.dtype, device=device)\n",
    "\n",
    "        for batch_idx, beam_hyp in enumerate(self._beam_hyps):\n",
    "            if self._done[batch_idx]:\n",
    "                if self.num_beams < len(beam_hyp):\n",
    "                    raise ValueError(f\"Batch can only be done if at least {self.num_beams} beams have been generated\")\n",
    "                if eos_token_id is None or pad_token_id is None:\n",
    "                    raise ValueError(\"Generated beams >= num_beams -> eos_token_id and pad_token have to be defined\")\n",
    "                # pad the batch\n",
    "                next_beam_scores[batch_idx, :] = 0\n",
    "                next_beam_tokens[batch_idx, :] = pad_token_id\n",
    "                next_beam_indices[batch_idx, :] = 0\n",
    "                continue\n",
    "\n",
    "            # next tokens for this sentence\n",
    "            beam_idx = 0\n",
    "            for beam_token_rank, (next_token, next_score, next_index) in enumerate(\n",
    "                zip(next_tokens[batch_idx], next_scores[batch_idx], next_indices[batch_idx])\n",
    "            ):\n",
    "                batch_beam_idx = batch_idx * self.group_size + next_index\n",
    "                # add to generated hypotheses if end of sentence\n",
    "                if (eos_token_id is not None) and (next_token.item() == eos_token_id):\n",
    "                    # if beam_token does not belong to top num_beams tokens, it should not be added\n",
    "                    is_beam_token_worse_than_top_num_beams = beam_token_rank >= self.group_size\n",
    "                    if is_beam_token_worse_than_top_num_beams:\n",
    "                        continue\n",
    "                    if beam_indices is not None:\n",
    "                        beam_index = beam_indices[batch_beam_idx]\n",
    "                        beam_index = beam_index + (batch_beam_idx,)\n",
    "                    else:\n",
    "                        beam_index = None\n",
    "\n",
    "                    beam_hyp.add(\n",
    "                        input_ids[batch_beam_idx].clone(),\n",
    "                        next_score.item(),\n",
    "                        beam_indices=beam_index\n",
    "                    )\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                else:\n",
    "                    # add next predicted token since it is not eos_token\n",
    "                    next_beam_scores[batch_idx, beam_idx] = next_score\n",
    "                    next_beam_tokens[batch_idx, beam_idx] = next_token\n",
    "                    next_beam_indices[batch_idx, beam_idx] = batch_beam_idx\n",
    "                    beam_idx += 1\n",
    "\n",
    "                # once the beam for next step is full, don't add more tokens to it.\n",
    "                if beam_idx == self.group_size:\n",
    "                    break\n",
    "\n",
    "            if beam_idx < self.group_size:\n",
    "                raise ValueError(\n",
    "                    f\"At most {self.group_size} tokens in {next_tokens[batch_idx]} can be equal to `eos_token_id:\"\n",
    "                    f\" {eos_token_id}`. Make sure {next_tokens[batch_idx]} are corrected.\"\n",
    "                )\n",
    "\n",
    "            # Check if we are done so that we can save a pad step if all(done)\n",
    "            self._done[batch_idx] = self._done[batch_idx] or beam_hyp.is_done(\n",
    "                next_scores[batch_idx].max().item(), cur_len\n",
    "            )\n",
    "        \n",
    "        return UserDict(\n",
    "            {\n",
    "                \"next_beam_scores\": next_beam_scores.view(-1),\n",
    "                \"next_beam_tokens\": next_beam_tokens.view(-1),\n",
    "                \"next_beam_indices\": next_beam_indices.view(-1),\n",
    "            }\n",
    "        )\n",
    "    @torch.no_grad()\n",
    "    def finalize(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        final_beam_scores: torch.FloatTensor,\n",
    "        final_beam_tokens: torch.LongTensor,\n",
    "        final_beam_indices: torch.LongTensor,\n",
    "        max_length: int,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[int] = None,\n",
    "        beam_indices: Optional[torch.LongTensor] = None,\n",
    "    ) -> Tuple[torch.LongTensor]:\n",
    "        batch_size = len(self._beam_hyps)\n",
    "\n",
    "        # finalize all open beam hypotheses and add to generated hypotheses\n",
    "        for batch_idx, beam_hyp in enumerate(self._beam_hyps):\n",
    "            if self._done[batch_idx]:\n",
    "                continue\n",
    "\n",
    "            # all open beam hypotheses are added to the beam hypothesis\n",
    "            # beam hypothesis class automatically keeps the best beams\n",
    "            for beam_id in range(self.num_beams):\n",
    "                batch_beam_idx = batch_idx * self.num_beams + beam_id\n",
    "                final_score = final_beam_scores[batch_beam_idx].item()\n",
    "                final_tokens = input_ids[batch_beam_idx]\n",
    "                beam_index = beam_indices[batch_beam_idx] if beam_indices is not None else None\n",
    "                beam_hyp.add(final_tokens, final_score, beam_indices=beam_index)\n",
    "\n",
    "        # select the best hypotheses\n",
    "        sent_lengths = input_ids.new(batch_size * self.num_beam_hyps_to_keep)\n",
    "        best = []\n",
    "        best_indices = []\n",
    "        best_scores = torch.zeros(batch_size * self.num_beam_hyps_to_keep, device=self.device, dtype=torch.float32)\n",
    "\n",
    "        # retrieve best hypotheses\n",
    "        for i, beam_hyp in enumerate(self._beam_hyps):\n",
    "            sorted_hyps = sorted(beam_hyp.beams, key=lambda x: x[0])\n",
    "            for j in range(self.num_beam_hyps_to_keep):\n",
    "                best_hyp_tuple = sorted_hyps.pop()\n",
    "                best_score = best_hyp_tuple[0]\n",
    "                best_hyp = best_hyp_tuple[1]\n",
    "                best_index = best_hyp_tuple[2]\n",
    "                sent_lengths[self.num_beam_hyps_to_keep * i + j] = len(best_hyp)\n",
    "\n",
    "                # append hyp to lists\n",
    "                best.append(best_hyp)\n",
    "\n",
    "                # append indices to list\n",
    "                best_indices.append(best_index)\n",
    "\n",
    "                best_scores[i * self.num_beam_hyps_to_keep + j] = best_score\n",
    "\n",
    "        # prepare for adding eos\n",
    "        sent_lengths_max = sent_lengths.max().item() + 1\n",
    "        sent_max_len = min(sent_lengths_max, max_length) if max_length is not None else sent_lengths_max\n",
    "        decoded: torch.LongTensor = input_ids.new(batch_size * self.num_beam_hyps_to_keep, sent_max_len)\n",
    "\n",
    "        if len(best_indices) > 0 and best_indices[0] is not None:\n",
    "            indices: torch.LongTensor = input_ids.new(batch_size * self.num_beam_hyps_to_keep, sent_max_len)\n",
    "        else:\n",
    "            indices = None\n",
    "\n",
    "        # shorter batches are padded if needed\n",
    "        if sent_lengths.min().item() != sent_lengths.max().item():\n",
    "            assert pad_token_id is not None, \"`pad_token_id` has to be defined\"\n",
    "            decoded.fill_(pad_token_id)\n",
    "\n",
    "        if indices is not None:\n",
    "            indices.fill_(-1)\n",
    "\n",
    "        # fill with hypotheses and eos_token_id if the latter fits in\n",
    "        for i, (hypo, best_idx) in enumerate(zip(best, best_indices)):\n",
    "            decoded[i, : sent_lengths[i]] = hypo\n",
    "\n",
    "            if indices is not None:\n",
    "                indices[i, : len(best_idx)] = torch.tensor(best_idx)\n",
    "\n",
    "            if sent_lengths[i] < sent_max_len:\n",
    "                decoded[i, sent_lengths[i]] = eos_token_id\n",
    "        \n",
    "        del self._done\n",
    "        torch.cuda.empty_cache()\n",
    "        return UserDict(\n",
    "            {\n",
    "                \"sequences\": decoded,\n",
    "                \"sequence_scores\": best_scores,\n",
    "                \"beam_indices\": indices,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "class BeamHypotheses:\n",
    "    def __init__(self, num_beams: int, length_penalty: float, early_stopping: bool, lambd: float, gamma: float,\n",
    "                 new_line_token: int, loop_tokens: list):\n",
    "        \"\"\"\n",
    "        Initialize n-best list of hypotheses.\n",
    "        \"\"\"\n",
    "        self.length_penalty = length_penalty\n",
    "        self.lambd = lambd\n",
    "        self.gamma = gamma\n",
    "        self.early_stopping = early_stopping\n",
    "        self.num_beams = num_beams\n",
    "        self.beams = []\n",
    "        self.worst_score = 1e9\n",
    "        self.new_line_token = new_line_token\n",
    "        self.loop_tokens = loop_tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of hypotheses in the list.\n",
    "        \"\"\"\n",
    "        return len(self.beams)\n",
    "    @torch.no_grad()\n",
    "    def add(self, hyp: torch.LongTensor, sum_logprobs: float, beam_indices: Optional[torch.LongTensor] = None):\n",
    "        \"\"\"\n",
    "        Add a new hypothesis to the list.\n",
    "        \"\"\"\n",
    "        loop_counts = 0\n",
    "        for tok in self.loop_tokens:\n",
    "            loop_counts += torch.numel(hyp[hyp == tok])\n",
    "            \n",
    "        score = (sum_logprobs * (1 + (self.lambd * torch.numel(hyp[hyp == self.new_line_token]))+ (self.gamma * loop_counts))) / (hyp.shape[-1] ** self.length_penalty)\n",
    "        #score = sum_logprobs / (hyp.shape[-1] ** self.length_penalty)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if len(self) < self.num_beams or score > self.worst_score:\n",
    "            self.beams.append((score, hyp, beam_indices))\n",
    "            if len(self) > self.num_beams:\n",
    "                sorted_next_scores = sorted([(s, idx) for idx, (s, _, _) in enumerate(self.beams)])\n",
    "                del self.beams[sorted_next_scores[0][1]]\n",
    "                self.worst_score = sorted_next_scores[1][0]\n",
    "                del score\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "            else:\n",
    "                self.worst_score = min(score, self.worst_score)\n",
    "                del score\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "    @torch.no_grad()\n",
    "    def is_done(self, best_sum_logprobs: float, cur_len: int) -> bool:\n",
    "        \"\"\"\n",
    "        If there are enough hypotheses and that none of the hypotheses being generated can become better than the worst\n",
    "        one in the heap, then we are done with this sentence.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self) < self.num_beams:\n",
    "            return False\n",
    "        elif self.early_stopping:\n",
    "            return True\n",
    "        else:\n",
    "            cur_score = best_sum_logprobs / cur_len**self.length_penalty\n",
    "            ret = self.worst_score >= cur_score\n",
    "            return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1666732704066,
     "user": {
      "displayName": "Brendan Kondracki",
      "userId": "14824001390510818675"
     },
     "user_tz": 240
    },
    "id": "VuAyt88o9AMb"
   },
   "outputs": [],
   "source": [
    "from transformers.utils import ModelOutput\n",
    "from transformers.generation_beam_constraints import Constraint\n",
    "from transformers import LogitsProcessorList, StoppingCriteriaList\n",
    "from transformers import BeamSearchScorer\n",
    "import inspect\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
    "@torch.no_grad()\n",
    "def generate_beam_search(\n",
    "    model,\n",
    "    inputs: Optional[torch.Tensor] = None,\n",
    "    max_length: Optional[int] = None,\n",
    "    min_length: Optional[int] = None,\n",
    "    early_stopping: Optional[bool] = None,\n",
    "    num_beams: Optional[int] = None,\n",
    "    repetition_penalty: Optional[float] = None,\n",
    "    bad_words_ids: Optional[Iterable[int]] = None,\n",
    "    force_words_ids: Optional[Union[Iterable[int], Iterable[Iterable[int]]]] = None,\n",
    "    bos_token_id: Optional[int] = None,\n",
    "    pad_token_id: Optional[int] = None,\n",
    "    eos_token_id: Optional[int] = None,\n",
    "    length_penalty: Optional[float] = None,\n",
    "    no_repeat_ngram_size: Optional[int] = None,\n",
    "    encoder_no_repeat_ngram_size: Optional[int] = None,\n",
    "    num_return_sequences: Optional[int] = None,\n",
    "    max_time: Optional[float] = None,\n",
    "    max_new_tokens: Optional[int] = None,\n",
    "    decoder_start_token_id: Optional[int] = None,\n",
    "    use_cache: Optional[bool] = None,\n",
    "    num_beam_groups: Optional[int] = None,\n",
    "    diversity_penalty: Optional[float] = None,\n",
    "    prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n",
    "    logits_processor: Optional[LogitsProcessorList] = LogitsProcessorList(),\n",
    "    renormalize_logits: Optional[bool] = None,\n",
    "    stopping_criteria: Optional[StoppingCriteriaList] = StoppingCriteriaList(),\n",
    "    constraints: Optional[List[Constraint]] = None,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    output_scores: Optional[bool] = None,\n",
    "    return_dict_in_generate: Optional[bool] = None,\n",
    "    forced_bos_token_id: Optional[int] = None,\n",
    "    forced_eos_token_id: Optional[int] = None,\n",
    "    remove_invalid_values: Optional[bool] = None,\n",
    "    synced_gpus: Optional[bool] = False,\n",
    "    exponential_decay_length_penalty: Optional[Tuple[int, float]] = None,\n",
    "    suppress_tokens: Optional[List[int]] = None,\n",
    "    begin_suppress_tokens: Optional[List[int]] = None,\n",
    "    forced_decoder_ids: Optional[List[int]] = None,\n",
    "    beam_scorer: Optional[BeamScorer] = None,\n",
    "    **model_kwargs,\n",
    "):\n",
    "    r\"\"\"\n",
    "    Parameters:\n",
    "        inputs (`torch.Tensor` of varying shape depending on the modality, *optional*):\n",
    "            The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the\n",
    "            method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n",
    "            should of in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of\n",
    "            `input_ids`, `input_values`, `input_features`, or `pixel_values`.\n",
    "        max_length (`int`, *optional*, defaults to `model.config.max_length`):\n",
    "            The maximum length the generated tokens can have. Corresponds to the length of the input prompt +\n",
    "            `max_new_tokens`. In general, prefer the use of `max_new_tokens`, which ignores the number of tokens in\n",
    "            the prompt.\n",
    "        max_new_tokens (`int`, *optional*):\n",
    "            The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "        min_length (`int`, *optional*, defaults to `model.config.min_length` or 10 if the config does not set any value):\n",
    "            The minimum length of the sequence to be generated.\n",
    "        early_stopping (`bool`, *optional*, defaults to `False`):\n",
    "            Whether to stop the beam search when at least `num_beams` sentences are finished per batch or not.\n",
    "        num_beams (`int`, *optional*, defaults to `model.config.num_beams` or 1 if the config does not set any value):\n",
    "            Number of beams for beam search. 1 means no beam search.\n",
    "        repetition_penalty (`float`, *optional*, defaults to `model.config.repetition_penalty` or 1.0 if the config does not set any value):\n",
    "            The parameter for repetition penalty. 1.0 means no penalty. See [this\n",
    "            paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.\n",
    "        pad_token_id (`int`, *optional*, defaults to `model.config.pad_token_id`):\n",
    "            The id of the *padding* token.\n",
    "        bos_token_id (`int`, *optional*, defaults to `model.config.bos_token_id`):\n",
    "            The id of the *beginning-of-sequence* token.\n",
    "        eos_token_id (`int`, *optional*, defaults to `model.config.eos_token_id`):\n",
    "            The id of the *end-of-sequence* token.\n",
    "        length_penalty (`float`, *optional*, defaults to `model.config.length_penalty` or 1.0 if the config does not set any value):\n",
    "            Exponential penalty to the length that is used with beam-based generation. It is applied as an exponent\n",
    "            to the sequence length, which in turn is used to divide the score of the sequence. Since the score is\n",
    "            the log likelihood of the sequence (i.e. negative), `length_penalty` > 0.0 promotes longer sequences,\n",
    "            while `length_penalty` < 0.0 encourages shorter sequences.\n",
    "        no_repeat_ngram_size (`int`, *optional*, defaults to `model.config.no_repeat_ngram_size` or 0 if the config does not set any value):\n",
    "            If set to int > 0, all ngrams of that size can only occur once.\n",
    "        encoder_no_repeat_ngram_size (`int`, *optional*, defaults to `model.config.encoder_no_repeat_ngram_size` or 0 if the config does not set any value):\n",
    "            If set to int > 0, all ngrams of that size that occur in the `encoder_input_ids` cannot occur in the\n",
    "            `decoder_input_ids`.\n",
    "        bad_words_ids(`List[List[int]]`, *optional*, defaults to `model.config.bad_words_ids`):\n",
    "            List of token ids that are not allowed to be generated. In order to get the token ids of the words that\n",
    "            should not appear in the generated text, use `tokenizer(bad_words, add_prefix_space=True,\n",
    "            add_special_tokens=False).input_ids`.\n",
    "        force_words_ids(`List[List[int]]` or `List[List[List[int]]]`, *optional*):\n",
    "            List of token ids that must be generated. If given a `List[List[int]]`, this is treated as a simple\n",
    "            list of words that must be included, the opposite to `bad_words_ids`. If given `List[List[List[int]]]`,\n",
    "            this triggers a [disjunctive constraint](https://github.com/huggingface/transformers/issues/14081),\n",
    "            where one can allow different forms of each word.\n",
    "        num_return_sequences(`int`, *optional*, defaults to `model.config.num_return_sequences` or 1 if the config does not set any value):\n",
    "            The number of independently computed returned sequences for each element in the batch.\n",
    "        max_time(`float`, *optional*):\n",
    "            The maximum amount of time you allow the computation to run for in seconds. generation will still\n",
    "            finish the current pass after allocated time has been passed.\n",
    "        attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on padding token indices. Mask values are in `[0, 1]`, 1 for tokens\n",
    "            that are not masked, and 0 for masked tokens. If not provided, will default to a tensor the same shape\n",
    "            as `input_ids` that masks the pad token. [What are attention masks?](../glossary#attention-mask)\n",
    "        decoder_start_token_id (`int`, *optional*):\n",
    "            If an encoder-decoder model starts decoding with a different token than *bos*, the id of that token.\n",
    "        use_cache (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not the model should use the past last key/values attentions (if applicable to the model) to\n",
    "            speed up decoding.\n",
    "        num_beam_groups (`int`, *optional*, defaults to `model.config.num_beam_groups` or 1 if the config does not set any value):\n",
    "            Number of groups to divide `num_beams` into in order to ensure diversity among different groups of\n",
    "            beams. [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.\n",
    "        diversity_penalty (`float`, *optional*, defaults to `model.config.diversity_penalty` or 0.0 if the config does not set any value):\n",
    "            This value is subtracted from a beam's score if it generates a token same as any beam from other group\n",
    "            at a particular time. Note that `diversity_penalty` is only effective if `group beam search` is\n",
    "            enabled.\n",
    "        prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n",
    "            If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
    "            provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n",
    "            `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n",
    "            on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n",
    "            for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n",
    "            Retrieval](https://arxiv.org/abs/2010.00904).\n",
    "        logits_processor (`LogitsProcessorList`, *optional*):\n",
    "              Custom logits processors that complement the default logits processors built from arguments and a\n",
    "              model's config. If a logit processor is passed that is already created with the arguments or a model's\n",
    "              config an error is thrown. This feature is intended for advanced users.\n",
    "        renormalize_logits: (`bool`, *optional*, defaults to `False`):\n",
    "            Whether to renormalize the logits after applying all the logits processors or warpers (including the\n",
    "            custom ones). It's highly recommended to set this flag to `True` as the search algorithms suppose the\n",
    "            score logits are normalized but some logit processors or warpers break the normalization.\n",
    "        stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
    "              Custom stopping criteria that complement the default stopping criteria built from arguments and a\n",
    "              model's config. If a stopping criteria is passed that is already created with the arguments or a\n",
    "              model's config an error is thrown. This feature is intended for advanced users.\n",
    "        constraints (`List[Constraint]`, *optional*):\n",
    "              Custom constraints that can be added to the generation to ensure that the output will contain the use\n",
    "              of certain tokens as defined by `Constraint` objects, in the most sensible way possible.\n",
    "        output_attentions (`bool`, *optional*, defaults to `model.config.output_attentions` or `False` if the config does not set any value):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "            returned tensors for more details.\n",
    "        output_hidden_states (`bool`, *optional*, defaults to `model.config.output_hidden_states` or `False` if the config does not set any value):\n",
    "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "            for more details.\n",
    "        output_scores (`bool`, *optional*, defaults to `model.config.output_scores` or `False` if the config does not set any value):\n",
    "            Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
    "        return_dict_in_generate (`bool`, *optional*, defaults to `model.config.return_dict_in_generate` or `False` if the config does not set any value):\n",
    "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "        forced_bos_token_id (`int`, *optional*, defaults to `model.config.forced_bos_token_id`):\n",
    "            The id of the token to force as the first generated token after the `decoder_start_token_id`. Useful\n",
    "            for multilingual models like [mBART](../model_doc/mbart) where the first generated token needs to be\n",
    "            the target language token.\n",
    "        forced_eos_token_id (`int`, *optional*, defaults to `model.config.forced_eos_token_id`):\n",
    "            The id of the token to force as the last generated token when `max_length` is reached.\n",
    "        remove_invalid_values (`bool`, *optional*, defaults to `model.config.remove_invalid_values`):\n",
    "            Whether to remove possible *nan* and *inf* outputs of the model to prevent the generation method to\n",
    "            crash. Note that using `remove_invalid_values` can slow down generation.\n",
    "        synced_gpus (`bool`, *optional*, defaults to `False`):\n",
    "            Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
    "        exponential_decay_length_penalty (`tuple(int, float)`, *optional*, defaults to `model.config.exponential_decay_length_penalty`):\n",
    "            This Tuple adds an exponentially increasing length penalty, after a certain amount of tokens have been\n",
    "            generated. The tuple shall consist of: `(start_index, decay_factor)` where `start_index` indicates\n",
    "            where penalty starts and `decay_factor` represents the factor of exponential decay\n",
    "        suppress_tokens  (`List[int]`, *optional*, defaults to `model.config.suppress_tokens`):\n",
    "            A list of tokens that will be supressed at generation. The `SupressTokens` logit processor will set\n",
    "            their log probs to `-inf` so that they are not sampled.\n",
    "        begin_suppress_tokens  (`List[int]`, *optional*, defaults to `model.config.begin_suppress_tokens`):\n",
    "            A list of tokens that will be supressed at the begining of the generation. The `SupressBeginTokens`\n",
    "            logit processor will set their log probs to `-inf` so that they are not sampled.\n",
    "        forced_decoder_ids (`List[int]`, *optional*, defaults to `model.config.forced_decoder_ids`):\n",
    "            A list of tokens that will be forced as beginning tokens, before sampling.\n",
    "        beam_scorer ('BeamScorer', *optional*)\n",
    "            Scorer to use for beam search generation\n",
    "        model_kwargs:\n",
    "            Additional model specific kwargs will be forwarded to the `forward` function of the model. If the model\n",
    "            is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs\n",
    "            should be prefixed with *decoder_*.\n",
    "    Return:\n",
    "        [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n",
    "        or when `config.return_dict_in_generate=True`) or a `torch.FloatTensor`.\n",
    "            If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n",
    "            [`~utils.ModelOutput`] types are:\n",
    "                - [`~generation_utils.GreedySearchDecoderOnlyOutput`],\n",
    "                - [`~generation_utils.SampleDecoderOnlyOutput`],\n",
    "                - [`~generation_utils.BeamSearchDecoderOnlyOutput`],\n",
    "                - [`~generation_utils.BeamSampleDecoderOnlyOutput`]\n",
    "            If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n",
    "            [`~utils.ModelOutput`] types are:\n",
    "                - [`~generation_utils.GreedySearchEncoderDecoderOutput`],\n",
    "                - [`~generation_utils.SampleEncoderDecoderOutput`],\n",
    "                - [`~generation_utils.BeamSearchEncoderDecoderOutput`],\n",
    "                - [`~generation_utils.BeamSampleEncoderDecoderOutput`]\n",
    "\"\"\"\n",
    "    # 0. Validate the `.generate()` call\n",
    "    model._validate_model_class()\n",
    "    model._validate_model_kwargs(model_kwargs.copy())\n",
    "\n",
    "    # 1. Set generation parameters if not already defined\n",
    "    bos_token_id = bos_token_id if bos_token_id is not None else model.config.bos_token_id\n",
    "    num_beams = num_beams if num_beams is not None else model.config.num_beams\n",
    "    length_penalty = length_penalty if length_penalty is not None else model.config.length_penalty\n",
    "    early_stopping = early_stopping if early_stopping is not None else model.config.early_stopping\n",
    "    num_beam_groups = num_beam_groups if num_beam_groups is not None else model.config.num_beam_groups\n",
    "    num_return_sequences = (\n",
    "        num_return_sequences if num_return_sequences is not None else model.config.num_return_sequences\n",
    "    )\n",
    "\n",
    "    pad_token_id = pad_token_id if pad_token_id is not None else model.config.pad_token_id\n",
    "    eos_token_id = eos_token_id if eos_token_id is not None else model.config.eos_token_id\n",
    "\n",
    "    if eos_token_id is None and hasattr(model.config, \"decoder\"):\n",
    "        eos_token_id = model.config.decoder.eos_token_id\n",
    "\n",
    "    if pad_token_id is None and eos_token_id is not None:\n",
    "        pad_token_id = eos_token_id\n",
    "\n",
    "    output_scores = output_scores if output_scores is not None else model.config.output_scores\n",
    "    output_attentions = output_attentions if output_attentions is not None else model.config.output_attentions\n",
    "    output_hidden_states = (\n",
    "        output_hidden_states if output_hidden_states is not None else model.config.output_hidden_states\n",
    "    )\n",
    "    return_dict_in_generate = (\n",
    "        return_dict_in_generate if return_dict_in_generate is not None else model.config.return_dict_in_generate\n",
    "    )\n",
    "\n",
    "    # 2. Define model inputs\n",
    "    # inputs_tensor has to be defined\n",
    "    # model_input_name is defined if model-specific keyword input is passed\n",
    "    # otherwise model_input_name is None\n",
    "    # all model-specific keyword inputs are removed from `model_kwargs`\n",
    "    inputs_tensor, model_input_name, model_kwargs = model._prepare_model_inputs(inputs, bos_token_id, model_kwargs)\n",
    "    batch_size = inputs_tensor.shape[0]\n",
    "\n",
    "    # 3. Define other model kwargs\n",
    "    model_kwargs[\"output_attentions\"] = output_attentions\n",
    "    model_kwargs[\"output_hidden_states\"] = output_hidden_states\n",
    "    model_kwargs[\"use_cache\"] = use_cache\n",
    "\n",
    "    accepts_attention_mask = \"attention_mask\" in set(inspect.signature(model.forward).parameters.keys())\n",
    "    requires_attention_mask = \"encoder_outputs\" not in model_kwargs\n",
    "\n",
    "    if model_kwargs.get(\"attention_mask\", None) is None and requires_attention_mask and accepts_attention_mask:\n",
    "        model_kwargs[\"attention_mask\"] = model._prepare_attention_mask_for_generation(\n",
    "            inputs_tensor, pad_token_id, eos_token_id\n",
    "        )\n",
    "\n",
    "    # decoder-only models should use left-padding for generation\n",
    "    if not model.config.is_encoder_decoder:\n",
    "        if pad_token_id is not None and torch.sum(inputs_tensor[:, -1] == pad_token_id) > 0:\n",
    "            logger.warning(\n",
    "                \"A decoder-only architecture is being used, but right-padding was detected! For correct \"\n",
    "                \"generation results, please set `padding_side='left'` when initializing the tokenizer.\"\n",
    "            )\n",
    "\n",
    "    if model.config.is_encoder_decoder and \"encoder_outputs\" not in model_kwargs:\n",
    "        # if model is encoder decoder encoder_outputs are created\n",
    "        # and added to `model_kwargs`\n",
    "        model_kwargs = model._prepare_encoder_decoder_kwargs_for_generation(\n",
    "            inputs_tensor, model_kwargs, model_input_name\n",
    "        )\n",
    "\n",
    "    # 4. Prepare `input_ids` which will be used for auto-regressive generation\n",
    "    if model.config.is_encoder_decoder:\n",
    "        input_ids = model._prepare_decoder_input_ids_for_generation(\n",
    "            batch_size,\n",
    "            decoder_start_token_id=decoder_start_token_id,\n",
    "            bos_token_id=bos_token_id,\n",
    "            model_kwargs=model_kwargs,\n",
    "            device=inputs_tensor.device,\n",
    "        )\n",
    "    else:\n",
    "        # if decoder-only then inputs_tensor has to be `input_ids`\n",
    "        input_ids = inputs_tensor\n",
    "\n",
    "    # 5. Prepare `max_length` depending on other stopping criteria.\n",
    "    input_ids_seq_length = input_ids.shape[-1]\n",
    "    if max_length is None and max_new_tokens is None:\n",
    "        warnings.warn(\n",
    "            \"Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to \"\n",
    "            f\"{self.config.max_length} (`self.config.max_length`). Controlling `max_length` via the config is \"\n",
    "            \"deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend \"\n",
    "            \"using `max_new_tokens` to control the maximum length of the generation.\",\n",
    "            UserWarning,\n",
    "        )\n",
    "    elif max_length is None and max_new_tokens is not None:\n",
    "        max_length = max_new_tokens + input_ids_seq_length\n",
    "    elif max_length is not None and max_new_tokens is not None:\n",
    "        raise ValueError(\n",
    "            \"Both `max_new_tokens` and `max_length` have been set but they serve the same purpose -- setting a\"\n",
    "            \" limit to the generated output length. Remove one of those arguments. Please refer to the\"\n",
    "            \" documentation for more information. \"\n",
    "            \"(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\"\n",
    "        )\n",
    "    # default to config if still None\n",
    "    max_length = max_length if max_length is not None else model.config.max_length\n",
    "    min_length = min_length if min_length is not None else model.config.min_length\n",
    "\n",
    "    if min_length is not None and min_length > max_length:\n",
    "        raise ValueError(\n",
    "            f\"Unfeasible length constraints: the minimum length ({min_length}) is larger than the maximum \"\n",
    "            f\"length ({max_length})\"\n",
    "        )\n",
    "    if input_ids_seq_length >= max_length:\n",
    "        print(input_ids_seq_length)\n",
    "        print(max_length)\n",
    "        input_ids_string = \"decoder_input_ids\" if model.config.is_encoder_decoder else \"input_ids\"\n",
    "        logger.warning(\n",
    "            f\"Input length of {input_ids_string} is {input_ids_seq_length}, but `max_length` is set to\"\n",
    "            f\" {max_length}. This can lead to unexpected behavior. You should consider increasing \"\n",
    "            \"`max_new_tokens`.\"\n",
    "        )\n",
    "\n",
    "    # 7. prepare distribution pre_processing samplers\n",
    "    logits_processor = model._get_logits_processor(\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        encoder_no_repeat_ngram_size=encoder_no_repeat_ngram_size,\n",
    "        input_ids_seq_length=input_ids_seq_length,\n",
    "        encoder_input_ids=inputs_tensor,\n",
    "        bad_words_ids=bad_words_ids,\n",
    "        min_length=min_length,\n",
    "        max_length=max_length,\n",
    "        eos_token_id=eos_token_id,\n",
    "        forced_bos_token_id=forced_bos_token_id,\n",
    "        forced_eos_token_id=forced_eos_token_id,\n",
    "        prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    "        num_beams=num_beams,\n",
    "        num_beam_groups=num_beam_groups,\n",
    "        diversity_penalty=diversity_penalty,\n",
    "        remove_invalid_values=remove_invalid_values,\n",
    "        exponential_decay_length_penalty=exponential_decay_length_penalty,\n",
    "        logits_processor=logits_processor,\n",
    "        renormalize_logits=renormalize_logits,\n",
    "        suppress_tokens=suppress_tokens,\n",
    "        begin_suppress_tokens=begin_suppress_tokens,\n",
    "        forced_decoder_ids=forced_decoder_ids,\n",
    "    )\n",
    "\n",
    "    # 8. prepare stopping criteria\n",
    "    stopping_criteria = model._get_stopping_criteria(\n",
    "        max_length=max_length, max_time=max_time, stopping_criteria=stopping_criteria\n",
    "    )\n",
    "\n",
    "    if num_return_sequences > num_beams:\n",
    "        raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n",
    "\n",
    "    if stopping_criteria.max_length is None:\n",
    "        raise ValueError(\"`max_length` needs to be a stopping_criteria for now.\")\n",
    "\n",
    "    # 10. prepare beam search scorer\n",
    "    if beam_scorer == None:\n",
    "        beam_scorer = BeamSearchScorer(\n",
    "            batch_size=batch_size,\n",
    "            num_beams=num_beams,\n",
    "            device=inputs_tensor.device,\n",
    "            length_penalty=length_penalty,\n",
    "            do_early_stopping=early_stopping,\n",
    "            num_beam_hyps_to_keep=num_return_sequences,\n",
    "        )\n",
    "    # 11. interleave input_ids with `num_beams` additional sequences per batch\n",
    "    input_ids, model_kwargs = model._expand_inputs_for_generation(\n",
    "        input_ids, expand_size=num_beams, is_encoder_decoder=model.config.is_encoder_decoder, **model_kwargs\n",
    "    )\n",
    "\n",
    "    # 12. run beam search\n",
    "    return model.beam_search(\n",
    "        input_ids,\n",
    "        beam_scorer,\n",
    "        logits_processor=logits_processor,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        pad_token_id=pad_token_id,\n",
    "        eos_token_id=eos_token_id,\n",
    "        output_scores=output_scores,\n",
    "        return_dict_in_generate=return_dict_in_generate,\n",
    "        synced_gpus=synced_gpus,\n",
    "        **model_kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END: COPIED FROM https://github.com/huggingface/transformers/blob/v4.25.1/src/transformers/generation/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "gpt_model = GPT2LMHeadModel.from_pretrained(\"1.5B\", torch_dtype=torch.float16, low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40346816,
     "status": "ok",
     "timestamp": 1666571169343,
     "user": {
      "displayName": "Brendan Kondracki",
      "userId": "14824001390510818675"
     },
     "user_tz": 240
    },
    "id": "oqxN6-6SHKUX",
    "outputId": "2a7e1c0f-20b3-4c05-8f9a-114ec9d9f10a"
   },
   "outputs": [],
   "source": [
    "# CodeT5 Beam Search Generations\n",
    "model.model.eval()\n",
    "model.model.to(device)\n",
    "loader = data.DataLoader(test_data, batch_size=1, num_workers=4)\n",
    "num_beams=4\n",
    "new_line_token = tokenizer.encode(\"\\n\", add_special_tokens=False)[0]\n",
    "loop_tokens = [tokenizer.encode(\"for\", add_special_tokens=False)[0], tokenizer.encode(\"while\", add_special_tokens=False)[0]]\n",
    "output_codes = {}\n",
    "lambdas = [0, 0.25, 0.75]\n",
    "gammas = [0, 1, 2]\n",
    "for l in lambdas:\n",
    "    for g in gammas:\n",
    "        for batch_idx, batch in enumerate(loader):\n",
    "            print(batch_idx)\n",
    "            if batch_idx >= 400:\n",
    "                break\n",
    "            beam_scorer = CodeBeamSearchScorer(\n",
    "              batch_size = 1,\n",
    "              num_beams = num_beams,\n",
    "              num_beam_hyps_to_keep = num_beams,\n",
    "              lambd = l,\n",
    "              gamma = g,\n",
    "              device=model.model.device,\n",
    "              new_line_token = new_line_token,\n",
    "              loop_tokens = loop_tokens\n",
    "            )\n",
    "            output_codes[batch_idx] = []\n",
    "            outs = generate_beam_search(model.model, batch[\"source_ids\"].to(device), \n",
    "                                          attention_mask=batch['source_mask'].to(device), \n",
    "                                          max_length=400, num_beams=num_beams, beam_scorer=beam_scorer)\n",
    "\n",
    "            dec = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
    "            for entry in dec:\n",
    "              output_codes[batch_idx].append(entry)\n",
    "\n",
    "        with open(\"results/all_codes_nl_interview_lambda_\"+ str(l).replace(\".\",\"\") +\"_gamma_\" +str(g).replace(\".\",\"\")+ \".json\", \"w\") as outfile:\n",
    "            output_json = json.dumps(output_codes)\n",
    "            outfile.write(output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CodeT5 Nucleus sampling generations\n",
    "model.model.eval()\n",
    "model.model.to(device)\n",
    "loader = data.DataLoader(test_data, batch_size=1, num_workers=4)\n",
    "output_codes = {}\n",
    "for batch_idx, batch in enumerate(loader):\n",
    "    if batch_idx >= 400:\n",
    "        break\n",
    "    outs = model.model.generate(\n",
    "                batch[\"source_ids\"].to(device), \n",
    "                attention_mask=batch['source_mask'].to(device),  \n",
    "                do_sample=True, \n",
    "                max_length=400, \n",
    "                top_p=0.95, \n",
    "                num_return_sequences = 6,\n",
    "            )\n",
    "    output_codes[batch_idx] = []\n",
    "    dec = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
    "    for entry in dec:\n",
    "      output_codes[batch_idx].append(entry.split(\"ANSWER:\\n\")[-1])\n",
    "\n",
    "with open(\"results/all_codes_nucleus_interview_095.json\", \"w\") as outfile:\n",
    "    output_json = json.dumps(output_codes)\n",
    "    outfile.write(output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT2 Beam Search Generations\n",
    "gpt_model.eval()\n",
    "gpt_model.to(device=device, dtype=torch.float16, non_blocking=False, memory_format=torch.preserve_format)\n",
    "loader = data.DataLoader(gpt_test_data, batch_size=1, num_workers=2)\n",
    "num_beams=4\n",
    "loop_tokens = [gpt_tokenizer.encode(\"for\", add_special_tokens=False)[0], gpt_tokenizer.encode(\"while\", add_special_tokens=False)[0]]\n",
    "new_line_token = gpt_tokenizer.encode(\"\\n\", add_special_tokens=False)[0]\n",
    "lambd_vals = [0, 0.25, 0.75]\n",
    "gamma_vals = [0, 1, 2]\n",
    "\n",
    "for l in lambd_vals:\n",
    "    for g in gamma_vals:\n",
    "        output_codes = {}\n",
    "        for batch_idx, batch in enumerate(loader):\n",
    "            if batch_idx >= 400:\n",
    "                break\n",
    "            beam_scorer = CodeBeamSearchScorer(\n",
    "              batch_size = 1,\n",
    "              num_beams = num_beams,\n",
    "              num_beam_hyps_to_keep = num_beams,\n",
    "              lambd = l,\n",
    "              gamma = g,\n",
    "              device=gpt_model.device,\n",
    "              new_line_token = new_line_token,\n",
    "              loop_tokens = loop_tokens\n",
    "            )\n",
    "            print(batch_idx)\n",
    "            output_codes[batch_idx] = []\n",
    "            outs = generate_beam_search(gpt_model, batch[\"source_ids\"][0].to(device), \n",
    "                                          max_length=400 + len(batch[\"source_ids\"][0][0].to(device)), num_beams=num_beams, beam_scorer=beam_scorer)\n",
    "\n",
    "            dec = [gpt_tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
    "            for entry in dec:\n",
    "              output_codes[batch_idx].append(entry.split(\"ANSWER:\\n\")[-1])\n",
    "\n",
    "            del batch\n",
    "            del beam_scorer\n",
    "            del outs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        with open(\"results/all_codes_gpt2_nl_interview_lambda_\"+ str(l).replace(\".\",\"\") +\"_gamma_\" +str(g).replace(\".\",\"\")+ \".json\", \"w\") as outfile:\n",
    "            output_json = json.dumps(output_codes)\n",
    "            outfile.write(output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT2 Nucleus sampling generations\n",
    "gpt_model.eval()\n",
    "gpt_model.to(device)\n",
    "loader = data.DataLoader(gpt_test_data, batch_size=1, num_workers=4)\n",
    "output_codes = {}\n",
    "for batch_idx, batch in enumerate(loader):\n",
    "    if batch_idx >= 400:\n",
    "        break\n",
    "    outs = gpt_model.generate(\n",
    "                batch[\"source_ids\"][0].to(device),  \n",
    "                do_sample=True, \n",
    "                max_length=400 + len(batch[\"source_ids\"][0][0].to(device)), \n",
    "                top_p=0.95, \n",
    "                num_return_sequences = 6,\n",
    "            )\n",
    "    output_codes[batch_idx] = []\n",
    "    dec = [gpt_tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
    "    for entry in dec:\n",
    "      output_codes[batch_idx].append(entry.split(\"ANSWER:\\n\")[-1])\n",
    "\n",
    "with open(\"results/all_codes_gpt2_nucleus_interview_095.json\", \"w\") as outfile:\n",
    "    output_json = json.dumps(output_codes)\n",
    "    outfile.write(output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_constraints(file_name):\n",
    "    cons_list = []\n",
    "    with open(file_name, 'r') as f:\n",
    "        for line in f:\n",
    "            cons = []\n",
    "            for concept in json.loads(line):\n",
    "                cons.append([f' {c}' for c in concept])\n",
    "            cons_list.append(cons)\n",
    "    return cons_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT2 Neurologic A* Ablation Over Heuristic Hyperparameter\n",
    "from neurologic_astar.generate import generate\n",
    "from neurologic_astar.utils import tokenize_constraints\n",
    "from neurologic_astar.lexical_constraints import init_batch\n",
    "\n",
    "gpt_model.eval()\n",
    "gpt_model.to(device=device, dtype=torch.float16, non_blocking=False, memory_format=torch.preserve_format)\n",
    "loader = data.DataLoader(gpt_test_data, batch_size=1, num_workers=2)\n",
    "loop_tokens = [gpt_tokenizer.encode(\"for\", add_special_tokens=False)[0], gpt_tokenizer.encode(\"while\", add_special_tokens=False)[0]]\n",
    "num_beams=4\n",
    "new_line_token = gpt_tokenizer.encode(\"\\n\", add_special_tokens=False)[0]\n",
    "alpha_vals = [.1, .25, .5, .75]\n",
    "\n",
    "for val in alpha_vals:\n",
    "    output_codes = {}\n",
    "    for batch_idx, batch in enumerate(loader):\n",
    "        if batch_idx >= 75:\n",
    "            break\n",
    "        print(batch_idx)\n",
    "        output_codes[batch_idx] = []\n",
    "\n",
    "        constraints_list = read_constraints(\"neurologic_astar/dummy_constraint.json\")\n",
    "        constraints_list = tokenize_constraints(gpt_tokenizer, constraints_list)\n",
    "        key_constraints_list = constraints_list\n",
    "        \n",
    "        eos_ids = [gpt_tokenizer.eos_token_id]\n",
    "\n",
    "        constraints = init_batch(raw_constraints=constraints_list,\n",
    "                         key_constraints=key_constraints_list,\n",
    "                         beam_size=num_beams,\n",
    "                         eos_id=eos_ids)\n",
    "\n",
    "        outs = generate(gpt_model, batch[\"source_ids\"][0].to(device), \n",
    "                                      max_length=400 + len(batch[\"source_ids\"][0][0].to(device)),\n",
    "                                      num_beams=num_beams, lambd=0, gamma=0, new_line_token = new_line_token,\n",
    "                                      loop_tokens=loop_tokens, look_ahead_step=5, look_ahead_width=1, constraints=constraints,\n",
    "                                      prune_factor=50, alpha=val, sat_tolerance=0,num_return_sequences=num_beams)\n",
    "\n",
    "        dec = [gpt_tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
    "        for entry in dec:\n",
    "          output_codes[batch_idx].append(entry.split(\"ANSWER:\\n\")[-1])\n",
    "\n",
    "        del batch\n",
    "        del outs\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    with open(\"results/all_codes_gpt2_astar_nl_interview_ablation_\"+ str(val).replace(\".\",\"\") +\".json\", \"w\") as outfile:\n",
    "        output_json = json.dumps(output_codes)\n",
    "        outfile.write(output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_codes_gpt2_astar_nl_interview_ablation_01_results.json\n",
      "-- % Runtime Errors: 0.8777609682299546\n",
      "-- % Compile Errors: 0.0007060010085728693\n",
      "-- Pass @ 1: 0.006921099466086613\n",
      "----- Avg lines: 13.56\n",
      "-- Pass @ 2: 0.007118845165117659\n",
      "----- Avg lines: 13.546666666666667\n",
      "-- Pass @ 4: 0.007712082262210797\n",
      "----- Avg lines: 13.586666666666666\n",
      "all_codes_gpt2_astar_nl_interview_ablation_025_results.json\n",
      "-- % Runtime Errors: 0.8773575491467918\n",
      "-- % Compile Errors: 0.0006985330805308851\n",
      "-- Pass @ 1: 0.006921099466086613\n",
      "----- Avg lines: 13.56\n",
      "-- Pass @ 2: 0.007118845165117659\n",
      "----- Avg lines: 13.546666666666667\n",
      "-- Pass @ 4: 0.007712082262210797\n",
      "----- Avg lines: 13.586666666666666\n",
      "all_codes_gpt2_astar_nl_interview_ablation_05_results.json\n",
      "-- % Runtime Errors: 0.8773575491467918\n",
      "-- % Compile Errors: 0.0006985330805308851\n",
      "-- Pass @ 1: 0.006921099466086613\n",
      "----- Avg lines: 13.56\n",
      "-- Pass @ 2: 0.007118845165117659\n",
      "----- Avg lines: 13.546666666666667\n",
      "-- Pass @ 4: 0.007712082262210797\n",
      "----- Avg lines: 13.586666666666666\n",
      "all_codes_gpt2_astar_nl_interview_ablation_075_results.json\n",
      "-- % Runtime Errors: 0.8773575491467918\n",
      "-- % Compile Errors: 0.0006985330805308851\n",
      "-- Pass @ 1: 0.006921099466086613\n",
      "----- Avg lines: 13.56\n",
      "-- Pass @ 2: 0.007118845165117659\n",
      "----- Avg lines: 13.546666666666667\n",
      "-- Pass @ 4: 0.007712082262210797\n",
      "----- Avg lines: 13.586666666666666\n"
     ]
    }
   ],
   "source": [
    "results_list = [\"all_codes_gpt2_astar_nl_interview_ablation_01_results.json\", \"all_codes_gpt2_astar_nl_interview_ablation_025_results.json\", \"all_codes_gpt2_astar_nl_interview_ablation_05_results.json\",\n",
    "              \"all_codes_gpt2_astar_nl_interview_ablation_075_results.json\"]\n",
    "\n",
    "codes_list = [\"all_codes_gpt2_astar_nl_interview_ablation_01.json\", \"all_codes_gpt2_astar_nl_interview_ablation_025.json\", \"all_codes_gpt2_astar_nl_interview_ablation_05.json\",\n",
    "              \"all_codes_gpt2_astar_nl_interview_ablation_075.json\"]\n",
    "\n",
    "dirs = []\n",
    "with open(\"test_dirs.json\", \"r\") as apps_file:\n",
    "    dirs = json.load(apps_file)\n",
    "    \n",
    "for y in range(len(results_list)):\n",
    "    result = results_list[y]\n",
    "    code = codes_list[y]\n",
    "    with open('results/' + result, 'r') as file:\n",
    "        with open ('results/' + code, 'r') as code_file:\n",
    "            data = json.load(file)\n",
    "            code_data = json.load(code_file)\n",
    "            total_tests = 0\n",
    "            total_evals = 0\n",
    "            total_runtime_err = 0\n",
    "            total_compile_err = 0\n",
    "            i=0\n",
    "            for key in data:\n",
    "                with open(\"APPS/test/\" + dirs[i] + \"/input_output.json\", \"r\") as curr_file:\n",
    "                    test_info = json.load(curr_file)\n",
    "                    total_tests += len(test_info[\"inputs\"])\n",
    "                    total_evals += sum([len(x) for x in data[key]])\n",
    "\n",
    "\n",
    "                    total_runtime_err += sum([x.count(-1) for x in data[key]])\n",
    "                    total_compile_err += sum([x.count(-2) for x in data[key]])\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            print(result)\n",
    "            print(\"-- % Runtime Errors: \" + str(total_runtime_err/(total_evals)))\n",
    "            print(\"-- % Compile Errors: \" + str(total_compile_err/(total_evals)))\n",
    "            \n",
    "            for beam in [1,2,4]:\n",
    "                total_passed = 0\n",
    "                total_nls = []\n",
    "                for key in data:\n",
    "                    seqs = data[key][:beam]\n",
    "                    true_counts = [x.count(True) for x in seqs]\n",
    "                    max_true = max(true_counts)\n",
    "                    best_code = code_data[key][true_counts.index(max_true)]\n",
    "                    total_nls.append(best_code.count(\"\\n\"))\n",
    "                    total_passed += max_true\n",
    "                print(\"-- Pass @ \" + str(beam) + \": \" + str(total_passed/total_tests))\n",
    "                print(\"----- Avg lines: \" + str(sum(total_nls)/len(total_nls)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#GPT2 Neurologic A* Beam Search Generations\n",
    "from neurologic_astar.generate import generate\n",
    "from neurologic_astar.utils import tokenize_constraints\n",
    "from neurologic_astar.lexical_constraints import init_batch\n",
    "\n",
    "gpt_model.eval()\n",
    "gpt_model.to(device=device, dtype=torch.float16, non_blocking=False, memory_format=torch.preserve_format)\n",
    "loader = data.DataLoader(gpt_test_data, batch_size=1, num_workers=2)\n",
    "num_beams=4\n",
    "loop_tokens = [gpt_tokenizer.encode(\"for\", add_special_tokens=False)[0], gpt_tokenizer.encode(\"while\", add_special_tokens=False)[0]]\n",
    "new_line_token = gpt_tokenizer.encode(\"\\n\", add_special_tokens=False)[0]\n",
    "lambd_vals = [0, 0.25, 0.75]\n",
    "gamma_vals = [0, 1, 2]\n",
    "\n",
    "for l in lambd_vals:\n",
    "    for g in gamma_vals:\n",
    "        output_codes = {}\n",
    "        for batch_idx, batch in enumerate(loader):\n",
    "            if batch_idx >= 400:\n",
    "                break\n",
    "            print(batch_idx)\n",
    "            output_codes[batch_idx] = []\n",
    "\n",
    "            constraints_list = read_constraints(\"neurologic_astar/dummy_constraint.json\")\n",
    "            constraints_list = tokenize_constraints(gpt_tokenizer, constraints_list)\n",
    "            key_constraints_list = constraints_list\n",
    "\n",
    "            eos_ids = [gpt_tokenizer.eos_token_id]\n",
    "\n",
    "            constraints = init_batch(raw_constraints=constraints_list,\n",
    "                             key_constraints=key_constraints_list,\n",
    "                             beam_size=num_beams,\n",
    "                             eos_id=eos_ids)\n",
    "\n",
    "            outs = generate(gpt_model, batch[\"source_ids\"][0].to(device), \n",
    "                                          max_length=400 + len(batch[\"source_ids\"][0][0].to(device)),\n",
    "                                          num_beams=num_beams, lambd=l, gamma=g, new_line_token = new_line_token,\n",
    "                                          loop_tokens=loop_tokens, look_ahead_step=5, look_ahead_width=1, constraints=constraints,\n",
    "                                          prune_factor=50, alpha=.5, sat_tolerance=0,num_return_sequences=num_beams)\n",
    "\n",
    "            dec = [gpt_tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
    "            for entry in dec:\n",
    "              output_codes[batch_idx].append(entry.split(\"ANSWER:\\n\")[-1])\n",
    "\n",
    "            del batch\n",
    "            del outs\n",
    "            del constraints\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        with open(\"results/all_codes_gpt2_astar_nl_interview_lambda_\"+ str(l).replace(\".\",\"\") +\"_gamma_\" +str(g).replace(\".\",\"\")+ \".json\", \"w\") as outfile:\n",
    "            output_json = json.dumps(output_codes)\n",
    "            outfile.write(output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save json file containing directory names of test data to be utilized\n",
    "import json\n",
    "test_dirs = []\n",
    "for sample in test_data.data:\n",
    "    test_dirs.append(str(sample[\"problem_id\"]).zfill(4))\n",
    "json_test_dirs = json.dumps(test_dirs)\n",
    "with open(\"test_dirs.json\", \"w\") as outfile:\n",
    "    outfile.write(json_test_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run test cases for solutions provided in file list and save results to json files\n",
    "import testing_util\n",
    "import test_one_solution\n",
    "import argparse\n",
    "\n",
    "file_list = [\"all_codes_nl_interview_0\", \"all_codes_nl_interview_025\", \"all_codes_nl_interview_075\", \"all_codes_nucleus_interview_095\",\n",
    "              \"all_codes_gpt2_nl_interview_0\", \"all_codes_gpt2_nl_interview_025\", \"all_codes_gpt2_nl_interview_075\", \"all_codes_gpt2_nucleus_interview_095\",\n",
    "              \"all_codes_gpt2_astar_nl_interview_0\", \"all_codes_gpt2_astar_nl_interview_025\", \"all_codes_gpt2_astar_nl_interview_075\",\n",
    "              \"all_codes_nl_interview_lambda_0_gamma_1\", \"all_codes_nl_interview_lambda_0_gamma_2\", \"all_codes_gpt2_nl_interview_lambda_0_gamma_1\",\n",
    "              \"all_codes_gpt2_nl_interview_lambda_0_gamma_2\", \"all_codes_gpt2_astar_nl_interview_lambda_0_gamma_1\", \"all_codes_gpt2_astar_nl_interview_lambda_0_gamma_2\"]\n",
    "for f in file_list:\n",
    "    parser = argparse.ArgumentParser(description=\"Testing a Language Model on Python Code\")\n",
    "    parser.add_argument(\"-t\",\"--test_loc\", default=\"../data_split/test.json\", type=str, help=\"path to the json containing problem paths to be evaluated.\")\n",
    "    parser.add_argument(\"-r\",\"--root\", default=\"../\", type=str, help=\"where the data is stored.\")\n",
    "    parser.add_argument(\"-s\",\"--start\", default=0, type=int)\n",
    "    parser.add_argument(\"-e\",\"--end\", default=None, type=int, help=\"If you want to evaluate a subset of problems specify start and ending index. File with start and ending prefix must exist typically used with batch evaluation.\")\n",
    "    parser.add_argument(\"-i\", \"--index\", default=0, type=int)\n",
    "    parser.add_argument(\"-p\", \"--print_results\", action=\"store_true\", help=\"If you have already evaluated the results and only want to print them.\")\n",
    "    parser.add_argument(\"-d\", \"--debug\", action=\"store_true\")\n",
    "    parser.add_argument(\"--save\", type=str, default=\"./results\", help=\"Where the evaluated data is loaded from and results saved to.\")\n",
    "    parser.add_argument(\"--stop-early\", default=None, type=int)\n",
    "    parser.add_argument(\"-c\", \"--codes_file\", default=\"all_codes.json\", type=str)\n",
    "    parser.add_argument(\"-f\", \"--save_file\", default=\"all_results.json\", type=str)\n",
    "    \n",
    "    code_f = f + \".json\"\n",
    "    save_f = f + \"_results.json\"\n",
    "    args = parser.parse_args(['--test_loc=test_dirs.json', '--root=APPS/test','--end=400', '--save=results', '--codes_file='+code_f, '--save_file='+save_f])\n",
    "    try:\n",
    "        test_one_solution.main(args)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_codes_nl_interview_0_results.json\n",
      "-- % Runtime Errors: 0.9194505878244674\n",
      "-- % Compile Errors: 0.005587242463042719\n",
      "-- Pass @ 1: 0.004188753382511028\n",
      "----- Avg lines: 13.1325\n",
      "----- Avg loops: 1.175\n",
      "-- Pass @ 2: 0.004893057048596953\n",
      "----- Avg lines: 13.1325\n",
      "----- Avg loops: 1.175\n",
      "-- Pass @ 4: 0.008970604589094414\n",
      "----- Avg lines: 13.1375\n",
      "----- Avg loops: 1.175\n",
      "\n",
      "********************\n",
      "\n",
      "all_codes_nl_interview_025_results.json\n",
      "-- % Runtime Errors: 0.9091266277503368\n",
      "-- % Compile Errors: 0.0034238886394252358\n",
      "-- Pass @ 1: 0.005189605960633132\n",
      "----- Avg lines: 6.3525\n",
      "----- Avg loops: 0.5575\n",
      "-- Pass @ 2: 0.007117173888868295\n",
      "----- Avg lines: 6.3525\n",
      "----- Avg loops: 0.5575\n",
      "-- Pass @ 4: 0.010416280535270786\n",
      "----- Avg lines: 6.3725\n",
      "----- Avg loops: 0.56\n",
      "\n",
      "********************\n",
      "\n",
      "all_codes_nl_interview_075_results.json\n",
      "-- % Runtime Errors: 0.9095208776446883\n",
      "-- % Compile Errors: 0.0032743759095488637\n",
      "-- Pass @ 1: 0.005189605960633132\n",
      "----- Avg lines: 6.295\n",
      "----- Avg loops: 0.5475\n",
      "-- Pass @ 2: 0.007117173888868295\n",
      "----- Avg lines: 6.295\n",
      "----- Avg loops: 0.5475\n",
      "-- Pass @ 4: 0.010416280535270786\n",
      "----- Avg lines: 6.315\n",
      "----- Avg loops: 0.55\n",
      "\n",
      "********************\n",
      "\n",
      "all_codes_nucleus_interview_095_results.json\n",
      "-- % Runtime Errors: 0.8476571042325203\n",
      "-- % Compile Errors: 0.013061370549293254\n",
      "-- Pass @ 1: 0.006931830818845683\n",
      "----- Avg lines: 12.345\n",
      "----- Avg loops: 1.27\n",
      "-- Pass @ 2: 0.009118879045112504\n",
      "----- Avg lines: 12.355\n",
      "----- Avg loops: 1.2625\n",
      "-- Pass @ 4: 0.014716239759795382\n",
      "----- Avg lines: 12.3725\n",
      "----- Avg loops: 1.255\n",
      "\n",
      "********************\n",
      "\n",
      "all_codes_gpt2_nl_interview_0_results.json\n",
      "-- % Runtime Errors: 0.839573086290053\n",
      "-- % Compile Errors: 0.0008356811995606704\n",
      "-- Pass @ 1: 0.00796975201097231\n",
      "----- Avg lines: 14.0\n",
      "----- Avg loops: 1.4175\n",
      "-- Pass @ 2: 0.00952663379916225\n",
      "----- Avg lines: 14.07\n",
      "----- Avg loops: 1.4225\n",
      "-- Pass @ 4: 0.011787819253438114\n",
      "----- Avg lines: 14.1475\n",
      "----- Avg loops: 1.425\n",
      "\n",
      "********************\n",
      "\n",
      "all_codes_gpt2_nl_interview_025_results.json\n",
      "-- % Runtime Errors: 0.8395729605515129\n",
      "-- % Compile Errors: 0.0008378016085790885\n",
      "-- Pass @ 1: 0.00796975201097231\n",
      "----- Avg lines: 14.0\n",
      "----- Avg loops: 1.4175\n",
      "-- Pass @ 2: 0.00952663379916225\n",
      "----- Avg lines: 14.07\n",
      "----- Avg loops: 1.4225\n",
      "-- Pass @ 4: 0.011787819253438114\n",
      "----- Avg lines: 14.1475\n",
      "----- Avg loops: 1.425\n",
      "\n",
      "********************\n",
      "\n",
      "all_codes_gpt2_nl_interview_075_results.json\n",
      "-- % Runtime Errors: 0.839573086290053\n",
      "-- % Compile Errors: 0.0008356811995606704\n",
      "-- Pass @ 1: 0.00796975201097231\n",
      "----- Avg lines: 14.0\n",
      "----- Avg loops: 1.4175\n",
      "-- Pass @ 2: 0.00952663379916225\n",
      "----- Avg lines: 14.07\n",
      "----- Avg loops: 1.4225\n",
      "-- Pass @ 4: 0.011787819253438114\n",
      "----- Avg lines: 14.1475\n",
      "----- Avg loops: 1.425\n",
      "\n",
      "********************\n",
      "\n",
      "all_codes_gpt2_nucleus_interview_095_results.json\n",
      "-- % Runtime Errors: 0.8599561255484306\n",
      "-- % Compile Errors: 0.0029024637192035098\n",
      "-- Pass @ 1: 0.005856841012714535\n",
      "----- Avg lines: 17.99\n",
      "----- Avg loops: 1.89\n",
      "-- Pass @ 2: 0.00889646736108537\n",
      "----- Avg lines: 18.06\n",
      "----- Avg loops: 1.885\n",
      "-- Pass @ 4: 0.01797827779219335\n",
      "----- Avg lines: 18.135\n",
      "----- Avg loops: 1.885\n",
      "\n",
      "********************\n",
      "\n",
      "all_codes_gpt2_astar_nl_interview_0_results.json\n",
      "-- % Runtime Errors: 0.839573086290053\n",
      "-- % Compile Errors: 0.0008356811995606704\n",
      "-- Pass @ 1: 0.00796975201097231\n",
      "----- Avg lines: 14.0\n",
      "----- Avg loops: 1.4175\n",
      "-- Pass @ 2: 0.00952663379916225\n",
      "----- Avg lines: 14.07\n",
      "----- Avg loops: 1.4225\n",
      "-- Pass @ 4: 0.011787819253438114\n",
      "----- Avg lines: 14.1475\n",
      "----- Avg loops: 1.425\n",
      "\n",
      "********************\n",
      "\n",
      "all_codes_gpt2_astar_nl_interview_025_results.json\n",
      "-- % Runtime Errors: 0.8401874521805662\n",
      "-- % Compile Errors: 0.0008607498087222647\n",
      "-- Pass @ 1: 0.007450791414908997\n",
      "----- Avg lines: 13.6825\n",
      "----- Avg loops: 1.4\n",
      "-- Pass @ 2: 0.009489565185157726\n",
      "----- Avg lines: 13.8025\n",
      "----- Avg loops: 1.4025\n",
      "-- Pass @ 4: 0.011936093709456204\n",
      "----- Avg lines: 13.9175\n",
      "----- Avg loops: 1.4075\n",
      "\n",
      "********************\n",
      "\n",
      "all_codes_gpt2_astar_nl_interview_075_results.json\n",
      "-- % Runtime Errors: 0.8401874521805662\n",
      "-- % Compile Errors: 0.0008607498087222647\n",
      "-- Pass @ 1: 0.007450791414908997\n",
      "----- Avg lines: 13.6825\n",
      "----- Avg loops: 1.4\n",
      "-- Pass @ 2: 0.009489565185157726\n",
      "----- Avg lines: 13.8025\n",
      "----- Avg loops: 1.4025\n",
      "-- Pass @ 4: 0.011936093709456204\n",
      "----- Avg lines: 13.9175\n",
      "----- Avg loops: 1.4075\n",
      "\n",
      "********************\n",
      "\n",
      "all_codes_nl_interview_lambda_0_gamma_1_results.json\n",
      "-- % Runtime Errors: 0.9163792840357677\n",
      "-- % Compile Errors: 0.007873775444807276\n",
      "-- Pass @ 1: 0.00422582199651555\n",
      "----- Avg lines: 95.73\n",
      "----- Avg loops: 0.5975\n",
      "-- Pass @ 2: 0.004930125662601475\n",
      "----- Avg lines: 95.73\n",
      "----- Avg loops: 0.5975\n",
      "-- Pass @ 4: 0.008636987063053712\n",
      "----- Avg lines: 95.7425\n",
      "----- Avg loops: 0.5975\n",
      "\n",
      "********************\n",
      "\n",
      "all_codes_nl_interview_lambda_0_gamma_2_results.json\n",
      "-- % Runtime Errors: 0.9104518484710178\n",
      "-- % Compile Errors: 0.007789441655256352\n",
      "-- Pass @ 1: 0.004262890610520073\n",
      "----- Avg lines: 88.6475\n",
      "----- Avg loops: 0.57\n",
      "-- Pass @ 2: 0.005523223486673833\n",
      "----- Avg lines: 88.6475\n",
      "----- Avg loops: 0.57\n",
      "-- Pass @ 4: 0.00923008488712607\n",
      "----- Avg lines: 88.66\n",
      "----- Avg loops: 0.57\n",
      "\n",
      "********************\n",
      "\n",
      "all_codes_gpt2_nl_interview_lambda_0_gamma_1_results.json\n",
      "-- % Runtime Errors: 0.8394725526730686\n",
      "-- % Compile Errors: 0.0008599684678228465\n",
      "-- Pass @ 1: 0.008043889238981354\n",
      "----- Avg lines: 13.985\n",
      "----- Avg loops: 1.365\n",
      "-- Pass @ 2: 0.009489565185157726\n",
      "----- Avg lines: 14.0325\n",
      "----- Avg loops: 1.3675\n",
      "-- Pass @ 4: 0.011787819253438114\n",
      "----- Avg lines: 14.12\n",
      "----- Avg loops: 1.3725\n",
      "\n",
      "********************\n",
      "\n",
      "all_codes_gpt2_nl_interview_lambda_0_gamma_2_results.json\n",
      "-- % Runtime Errors: 0.8394721716639525\n",
      "-- % Compile Errors: 0.0008621515470830539\n",
      "-- Pass @ 1: 0.008043889238981354\n",
      "----- Avg lines: 13.985\n",
      "----- Avg loops: 1.365\n",
      "-- Pass @ 2: 0.009489565185157726\n",
      "----- Avg lines: 14.0325\n",
      "----- Avg loops: 1.3675\n",
      "-- Pass @ 4: 0.011787819253438114\n",
      "----- Avg lines: 14.12\n",
      "----- Avg loops: 1.3725\n",
      "\n",
      "********************\n",
      "\n",
      "all_codes_gpt2_astar_nl_interview_lambda_0_gamma_1_results.json\n",
      "-- % Runtime Errors: 0.8284430851572206\n",
      "-- % Compile Errors: 0.0007884927840963395\n",
      "-- Pass @ 1: 0.008266300923008488\n",
      "----- Avg lines: 13.9025\n",
      "----- Avg loops: 1.34\n",
      "-- Pass @ 2: 0.009674908255180338\n",
      "----- Avg lines: 13.9825\n",
      "----- Avg loops: 1.3425\n",
      "-- Pass @ 4: 0.011639544797420024\n",
      "----- Avg lines: 14.0325\n",
      "----- Avg loops: 1.345\n",
      "\n",
      "********************\n",
      "\n",
      "all_codes_gpt2_astar_nl_interview_lambda_0_gamma_2_results.json\n",
      "-- % Runtime Errors: 0.8284430851572206\n",
      "-- % Compile Errors: 0.0007884927840963395\n",
      "-- Pass @ 1: 0.008266300923008488\n",
      "----- Avg lines: 13.9025\n",
      "----- Avg loops: 1.34\n",
      "-- Pass @ 2: 0.009674908255180338\n",
      "----- Avg lines: 13.9825\n",
      "----- Avg loops: 1.3425\n",
      "-- Pass @ 4: 0.011639544797420024\n",
      "----- Avg lines: 14.0325\n",
      "----- Avg loops: 1.345\n",
      "\n",
      "********************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Gather summary statistics for model generated solutions\n",
    "\n",
    "#List of all json files containing test case results\n",
    "results_list = [\"all_codes_nl_interview_0_results.json\", \"all_codes_nl_interview_025_results.json\", \"all_codes_nl_interview_075_results.json\", \"all_codes_nucleus_interview_095_results.json\",\n",
    "                \"all_codes_gpt2_nl_interview_0_results.json\", \"all_codes_gpt2_nl_interview_025_results.json\", \"all_codes_gpt2_nl_interview_075_results.json\", \"all_codes_gpt2_nucleus_interview_095_results.json\",\n",
    "                \"all_codes_gpt2_astar_nl_interview_0_results.json\", \"all_codes_gpt2_astar_nl_interview_025_results.json\", \"all_codes_gpt2_astar_nl_interview_075_results.json\",\n",
    "                \"all_codes_nl_interview_lambda_0_gamma_1_results.json\", \"all_codes_nl_interview_lambda_0_gamma_2_results.json\", \"all_codes_gpt2_nl_interview_lambda_0_gamma_1_results.json\",\n",
    "                \"all_codes_gpt2_nl_interview_lambda_0_gamma_2_results.json\", \"all_codes_gpt2_astar_nl_interview_lambda_0_gamma_1_results.json\", \"all_codes_gpt2_astar_nl_interview_lambda_0_gamma_2_results.json\"]\n",
    "\n",
    "#List of all json files containing model generated programs\n",
    "codes_list = [\"all_codes_nl_interview_0.json\", \"all_codes_nl_interview_025.json\", \"all_codes_nl_interview_075.json\", \"all_codes_nucleus_interview_095.json\",\n",
    "              \"all_codes_gpt2_nl_interview_0.json\", \"all_codes_gpt2_nl_interview_025.json\", \"all_codes_gpt2_nl_interview_075.json\", \"all_codes_gpt2_nucleus_interview_095.json\",\n",
    "              \"all_codes_gpt2_astar_nl_interview_0.json\", \"all_codes_gpt2_astar_nl_interview_025.json\", \"all_codes_gpt2_astar_nl_interview_075.json\",\n",
    "              \"all_codes_nl_interview_lambda_0_gamma_1.json\", \"all_codes_nl_interview_lambda_0_gamma_2.json\", \"all_codes_gpt2_nl_interview_lambda_0_gamma_1.json\",\n",
    "              \"all_codes_gpt2_nl_interview_lambda_0_gamma_2.json\", \"all_codes_gpt2_astar_nl_interview_lambda_0_gamma_1.json\", \"all_codes_gpt2_astar_nl_interview_lambda_0_gamma_2.json\"]\n",
    "\n",
    "dirs = []\n",
    "with open(\"test_dirs.json\", \"r\") as apps_file:\n",
    "    dirs = json.load(apps_file)\n",
    "    \n",
    "for y in range(len(results_list)):\n",
    "    result = results_list[y]\n",
    "    code = codes_list[y]\n",
    "    with open('results/' + result, 'r') as file:\n",
    "        with open ('results/' + code, 'r') as code_file:\n",
    "            data = json.load(file)\n",
    "            code_data = json.load(code_file)\n",
    "            total_tests = 0\n",
    "            total_evals = 0\n",
    "            total_runtime_err = 0\n",
    "            total_compile_err = 0\n",
    "            i=0\n",
    "            for key in data:\n",
    "                with open(\"APPS/test/\" + dirs[i] + \"/input_output.json\", \"r\") as curr_file:\n",
    "                    test_info = json.load(curr_file)\n",
    "                    total_tests += len(test_info[\"inputs\"])\n",
    "                    total_evals += sum([len(x) for x in data[key]])\n",
    "\n",
    "\n",
    "                    total_runtime_err += sum([x.count(-1) for x in data[key]])\n",
    "                    total_compile_err += sum([x.count(-2) for x in data[key]])\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            print(result)\n",
    "            #Print percentage of runtime and compile errors from current json file\n",
    "            print(\"-- % Runtime Errors: \" + str(total_runtime_err/(total_evals)))\n",
    "            print(\"-- % Compile Errors: \" + str(total_compile_err/(total_evals)))\n",
    "            \n",
    "            #Calculate the percentage of test cases passed, as well as avg number of lines and loops\n",
    "            #for candidate solution sets of size 1,2,4\n",
    "            for beam in [1,2,4]:\n",
    "                total_passed = 0\n",
    "                total_nls = []\n",
    "                total_loops = []\n",
    "                for key in data:\n",
    "                    seqs = data[key][:beam]\n",
    "                    true_counts = [x.count(True) for x in seqs]\n",
    "                    max_true = max(true_counts)\n",
    "                    best_code = code_data[key][true_counts.index(max_true)]\n",
    "                    total_nls.append(best_code.count(\"\\n\"))\n",
    "                    total_loops.append(best_code.count(\"for\") + best_code.count(\"while\"))\n",
    "                    total_passed += max_true\n",
    "                print(\"-- Pass @ \" + str(beam) + \": \" + str(total_passed/total_tests))\n",
    "                print(\"----- Avg lines: \" + str(sum(total_nls)/len(total_nls)))\n",
    "                print(\"----- Avg loops: \" + str(sum(total_loops)/len(total_loops)))\n",
    "\n",
    "            print(\"\\n********************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_codes_nl_interview_0.json: 0.2373582374276232\n",
      "all_codes_nl_interview_025.json: 0.2346343947542429\n",
      "all_codes_nl_interview_075.json: 0.23406250887693003\n",
      "all_codes_nucleus_interview_095.json: 0.2885924421515881\n",
      "all_codes_gpt2_nl_interview_0.json: 0.2901704285958109\n",
      "all_codes_gpt2_nl_interview_025.json: 0.2901704285958109\n",
      "all_codes_gpt2_nl_interview_075.json: 0.2901704285958109\n",
      "all_codes_gpt2_nucleus_interview_095.json: 0.31954536236792686\n",
      "all_codes_gpt2_astar_nl_interview_0.json: 0.2906287576009002\n",
      "all_codes_gpt2_astar_nl_interview_025.json: 0.2894565783988229\n",
      "all_codes_gpt2_astar_nl_interview_075.json: 0.2894326501847852\n",
      "all_codes_nl_interview_lambda_0_gamma_1.json: 0.23522113290925048\n",
      "all_codes_nl_interview_lambda_0_gamma_2.json: 0.2351692226766906\n",
      "all_codes_gpt2_nl_interview_lambda_0_gamma_1.json: 0.2889940632644226\n",
      "all_codes_gpt2_nl_interview_lambda_0_gamma_2.json: 0.2889250361321371\n",
      "all_codes_gpt2_astar_nl_interview_lambda_0_gamma_1.json: 0.289503791498633\n",
      "all_codes_gpt2_astar_nl_interview_lambda_0_gamma_2.json: 0.2894347643663474\n"
     ]
    }
   ],
   "source": [
    "#Calculate the CodeBLEU scores for generated solutions, using sample solutions from APPS data as references\n",
    "import CodeBLEU.bleu as bleu\n",
    "import CodeBLEU.weighted_ngram_match as weighted_ngram_match\n",
    "import CodeBLEU.syntax_match as syntax_match\n",
    "import CodeBLEU.dataflow_match as dataflow_match\n",
    "import json\n",
    "\n",
    "lang = \"python\"\n",
    "alpha,beta,gamma,theta = [0.1, 0.1, 0.4, 0.4]\n",
    "\n",
    "codes_list = [\"all_codes_nl_interview_0.json\", \"all_codes_nl_interview_025.json\", \"all_codes_nl_interview_075.json\", \"all_codes_nucleus_interview_095.json\",\n",
    "              \"all_codes_gpt2_nl_interview_0.json\", \"all_codes_gpt2_nl_interview_025.json\", \"all_codes_gpt2_nl_interview_075.json\", \"all_codes_gpt2_nucleus_interview_095.json\",\n",
    "              \"all_codes_gpt2_astar_nl_interview_0.json\", \"all_codes_gpt2_astar_nl_interview_025.json\", \"all_codes_gpt2_astar_nl_interview_075.json\",\n",
    "              \"all_codes_nl_interview_lambda_0_gamma_1.json\", \"all_codes_nl_interview_lambda_0_gamma_2.json\", \"all_codes_gpt2_nl_interview_lambda_0_gamma_1.json\",\n",
    "              \"all_codes_gpt2_nl_interview_lambda_0_gamma_2.json\", \"all_codes_gpt2_astar_nl_interview_lambda_0_gamma_1.json\", \"all_codes_gpt2_astar_nl_interview_lambda_0_gamma_2.json\"]\n",
    "\n",
    "for code in codes_list:\n",
    "    with open(\"test_dirs.json\") as test_json:\n",
    "        with open(\"results/\" + code) as code_json:\n",
    "            json_data = json.load(test_json)\n",
    "            code_data = json.load(code_json)\n",
    "            \n",
    "            best_scores = 0\n",
    "            total_tests = 400\n",
    "            for i in range(400):\n",
    "                with open(\"APPS/test/\" + json_data[i] + \"/solutions.json\") as sol_json:\n",
    "                    solution_data = json.load(sol_json)\n",
    "                    pre_references = [[reference] \\\n",
    "                                    for reference in solution_data]\n",
    "                    generations = code_data[str(i)]\n",
    "                    \n",
    "                    if len(solution_data) == 0:\n",
    "                        total_tests -= 1\n",
    "                        continue\n",
    "                    \n",
    "                    best_score = 0\n",
    "                    for j in range(4):\n",
    "                        hypothesis = [generations[j]]\n",
    "                        \n",
    "                        references = []\n",
    "                        for i in range(len(hypothesis)):\n",
    "                            ref_for_instance = []\n",
    "                            for j in range(len(pre_references)):\n",
    "                                ref_for_instance.append(pre_references[j][i])\n",
    "                            references.append(ref_for_instance)\n",
    "\n",
    "\n",
    "                        # calculate ngram match (BLEU)\n",
    "                        tokenized_hyps = [x.split() for x in hypothesis]\n",
    "                        tokenized_refs = [[x.split() for x in reference] for reference in references]\n",
    "\n",
    "                        ngram_match_score = bleu.corpus_bleu(tokenized_refs,tokenized_hyps)\n",
    "\n",
    "                        # calculate weighted ngram match\n",
    "                        keywords = [x.strip() for x in open('CodeBLEU/keywords/'+lang+'.txt', 'r', encoding='utf-8').readlines()]\n",
    "                        def make_weights(reference_tokens, key_word_list):\n",
    "                            return {token:1 if token in key_word_list else 0.2 \\\n",
    "                                    for token in reference_tokens}\n",
    "                        tokenized_refs_with_weights = [[[reference_tokens, make_weights(reference_tokens, keywords)]\\\n",
    "                                    for reference_tokens in reference] for reference in tokenized_refs]\n",
    "                        weighted_ngram_match_score = weighted_ngram_match.corpus_bleu(tokenized_refs_with_weights,tokenized_hyps)\n",
    "\n",
    "                        # calculate syntax match\n",
    "                        syntax_match_score = syntax_match.corpus_syntax_match(references, hypothesis, lang)\n",
    "\n",
    "                        # calculate dataflow match\n",
    "                        dataflow_match_score = dataflow_match.corpus_dataflow_match(references, hypothesis, lang)\n",
    "\n",
    "                        code_bleu_score = alpha*ngram_match_score\\\n",
    "                                        + beta*weighted_ngram_match_score\\\n",
    "                                        + gamma*syntax_match_score\\\n",
    "                                        + theta*dataflow_match_score\n",
    "                        if code_bleu_score > best_score:\n",
    "                            best_score = code_bleu_score\n",
    "                            \n",
    "                    best_scores += best_score\n",
    "                    \n",
    "            print(code + \": \" + str(best_scores/total_tests))\n",
    "                \n",
    "                        \n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate sample programs from CodeT5 and GPT-2\n",
    "from neurologic_astar.generate import generate\n",
    "from neurologic_astar.utils import tokenize_constraints\n",
    "from neurologic_astar.lexical_constraints import init_batch\n",
    "\n",
    "model.model.eval()\n",
    "model.model.to(device)\n",
    "\n",
    "gpt_model.eval()\n",
    "gpt_model.to(device=device, dtype=torch.float16, non_blocking=False, memory_format=torch.preserve_format)\n",
    "\n",
    "loader = data.DataLoader(test_data, batch_size=1, num_workers=4)\n",
    "gpt_loader = data.DataLoader(gpt_test_data, batch_size=1, num_workers=2)\n",
    "\n",
    "new_line_token = tokenizer.encode(\"\\n\", add_special_tokens=False)[0]\n",
    "loop_tokens = [tokenizer.encode(\"for\", add_special_tokens=False)[0], tokenizer.encode(\"while\", add_special_tokens=False)[0]]\n",
    "\n",
    "gpt_loop_tokens = [gpt_tokenizer.encode(\"for\", add_special_tokens=False)[0], gpt_tokenizer.encode(\"while\", add_special_tokens=False)[0]]\n",
    "gpt_new_line_token = gpt_tokenizer.encode(\"\\n\", add_special_tokens=False)[0]\n",
    "\n",
    "codet5_sample = None\n",
    "gpt2_sample_beam = None\n",
    "gpt2_sample_astar = None\n",
    "\n",
    "for batch_idx, batch in enumerate(loader):\n",
    "    if batch_idx == 318:\n",
    "        \n",
    "        outs = model.model.generate(\n",
    "                    batch[\"source_ids\"].to(device), \n",
    "                    attention_mask=batch['source_mask'].to(device),  \n",
    "                    do_sample=True, \n",
    "                    max_length=400, \n",
    "                    top_p=0.95, \n",
    "                    num_return_sequences = 1,\n",
    "                )\n",
    "\n",
    "\n",
    "        dec = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
    "        codet5_sample = dec[0]               \n",
    "        break\n",
    "        \n",
    "for batch_idx, batch in enumerate(gpt_loader):\n",
    "    if batch_idx == 318:\n",
    "        constraints_list = read_constraints(\"neurologic_astar/dummy_constraint.json\")\n",
    "        constraints_list = tokenize_constraints(gpt_tokenizer, constraints_list)\n",
    "        key_constraints_list = constraints_list\n",
    "\n",
    "        eos_ids = [gpt_tokenizer.eos_token_id]\n",
    "\n",
    "        constraints = init_batch(raw_constraints=constraints_list,\n",
    "                         key_constraints=key_constraints_list,\n",
    "                         beam_size=4,\n",
    "                         eos_id=eos_ids)\n",
    "\n",
    "        outs = generate(gpt_model, batch[\"source_ids\"][0].to(device), \n",
    "                                      max_length=400 + len(batch[\"source_ids\"][0][0].to(device)),\n",
    "                                      num_beams=4, lambd=0.75, gamma=0, new_line_token = gpt_new_line_token,\n",
    "                                      loop_tokens=gpt_loop_tokens, look_ahead_step=5, look_ahead_width=1, constraints=constraints,\n",
    "                                      prune_factor=50, alpha=.5, sat_tolerance=0,num_return_sequences=4)\n",
    "        \n",
    "        beam_scorer = CodeBeamSearchScorer(\n",
    "              batch_size = 1,\n",
    "              num_beams = 4,\n",
    "              num_beam_hyps_to_keep = 4,\n",
    "              lambd = 0,\n",
    "              gamma = 0,\n",
    "              device=gpt_model.device,\n",
    "              new_line_token = gpt_new_line_token,\n",
    "              loop_tokens = gpt_loop_tokens\n",
    "            )\n",
    "\n",
    "        outs_beam = generate_beam_search(gpt_model, batch[\"source_ids\"][0].to(device), \n",
    "                                      max_length=400 + len(batch[\"source_ids\"][0][0].to(device)), num_beams=4, beam_scorer=beam_scorer)\n",
    "\n",
    "        dec = [gpt_tokenizer.decode(ids, skip_special_tokens=True).split(\"ANSWER:\\n\")[-1] for ids in outs]\n",
    "        dec_beam = [gpt_tokenizer.decode(ids, skip_special_tokens=True).split(\"ANSWER:\\n\")[-1] for ids in outs_beam]\n",
    "        \n",
    "        gpt2_sample_astar = dec[3]   \n",
    "        gpt2_sample_beam = dec_beam[3]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sys\n",
      "\n",
      "def main():\n",
      "\tn, m = map(int, sys.stdin.readline().split())\n",
      "\tww = [list(map(int, sys.stdin.readline().split())) for _ in range(n)]\n",
      "\tww.sort(key=lambda x: x[0], reverse=True)\n",
      "\tm = ww[0][0]\n",
      "\tans = 0\n",
      "\tfor w, c in ww:\n",
      "\t\tif c > m:\n",
      "\t\t\tbreak\n",
      "\t\tans += c\n",
      "\tprint(ans)\n",
      "\n",
      "\n",
      "def __starting_point():\n",
      "\tmain()\n",
      "\n",
      "__starting_point()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gpt2_sample_beam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sys\n",
      "\n",
      "def main():\n",
      "\tn, m = map(int, sys.stdin.readline().split())\n",
      "\tww = [list(map(int, sys.stdin.readline().split())) for _ in range(n)]\n",
      "\tww.sort(key=lambda x: x[0], reverse=True)\n",
      "\tans = 0\n",
      "\tfor w, c in ww:\n",
      "\t\tif c > m:\n",
      "\t\t\tbreak\n",
      "\t\tans += c\n",
      "\tprint(ans)\n",
      "\n",
      "main()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gpt2_sample_astar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n, m = map(int, input().split())\n",
      "a = [int(x) for x in input().split()]\n",
      "d = [[0] * n for x in range(m)]\n",
      "b = [0] * n\n",
      "d[0][n] = sum(a)\n",
      "#print(d[1])\n",
      "#print(d[1][0])\n",
      "for i in range(1, n+1):\n",
      "    a[i][i] = max(d[i][0], a[i-1][i])\n",
      "for i in range(n):\n",
      "    a[i][i] += d[i-1][i-1]\n",
      "print(b[n][m])\n"
     ]
    }
   ],
   "source": [
    "print(codet5_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02b89793ece04cb882adc21c6b5017df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "02dbb05070de4fb99038f3c1459d971b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "04ae4ba4bb3e4f1c9d181303a74dd901": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "067106e33595486282ab1c67a878093e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_342b2d59758743d18e8b7be77f47d2cf",
       "IPY_MODEL_a78807ba67b34e2b8f306e49603e895b",
       "IPY_MODEL_28ea636469a74af2916baa976379c761"
      ],
      "layout": "IPY_MODEL_a7c3e7b8ade14c41b77f9d3bd357dddb"
     }
    },
    "070ed550a6ba4a7aaaeb3ba15c4066a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "09d7f2349f034892999122bc3f163d39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0c316286f2234849a2ea62780ecb72bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_840ef064a4834daaa8a36ef53b0d6fff",
      "placeholder": "​",
      "style": "IPY_MODEL_1de1621ea92f4ad6ace210c58ec50f6a",
      "value": " 1.57k/1.57k [00:00&lt;00:00, 17.9kB/s]"
     }
    },
    "0cff02e713b7475f90e86a511a13e27b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0dc81039a8a44c92b01c620b318661f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62eba812c1ea420bbba424761ae04512",
      "placeholder": "​",
      "style": "IPY_MODEL_4cefa04beb45437b82585dce4f2dc07a",
      "value": " 703k/703k [00:00&lt;00:00, 3.63MB/s]"
     }
    },
    "0eff06df5d6641ee8ac379e31ac59fda": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f3887f816b94e22bb8a6af06ec4c0af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "12535537221449a08b56ffad4da68b74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b20913079b8d4e6f8acc14a97e4c1302",
      "placeholder": "​",
      "style": "IPY_MODEL_13fca227fa7b455aa9c015fa10e99731",
      "value": " 1.48k/1.48k [00:00&lt;00:00, 28.0kB/s]"
     }
    },
    "13fca227fa7b455aa9c015fa10e99731": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1499dca31374425e90ca96e0a8f0eb4c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15a85533473a441fb198aa9b6d8f1109": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17aeb3f915f14cbb80287500a1962a29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "181442a9ff0547d28f4aa33423b8c1b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "19fbd956215a4b8987f1fab9e1cc4a75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1beab5cdb547417aaa9f000456318095": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_227027e946804af082abb9b90053314e",
      "placeholder": "​",
      "style": "IPY_MODEL_b5d96b10092e4377b95d072e56f16ac5",
      "value": "Generating train split: "
     }
    },
    "1de1621ea92f4ad6ace210c58ec50f6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "21068c7def7f430aa7dde408bb4bbcf8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_365d539c8d904e84869d87cfe4826776",
      "placeholder": "​",
      "style": "IPY_MODEL_8ff46e04f6ab4ed2869bc3895269d9a0",
      "value": "Downloading: 100%"
     }
    },
    "227027e946804af082abb9b90053314e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23814bd71b2b41efb2cb30f0d9026d1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ae819c352f1436e8a8edbf070b614a1",
      "placeholder": "​",
      "style": "IPY_MODEL_517071c32d1540f987393c62c9ae7fda",
      "value": "Downloading: 100%"
     }
    },
    "2678115b6c0543a48db475aa836ad3d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "277ebcdd32df4b8db9085d9594e2fae5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "287849c76dd54795a248258bc97418eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_23814bd71b2b41efb2cb30f0d9026d1d",
       "IPY_MODEL_d4d35361775c4f08af96044d6641e3cd",
       "IPY_MODEL_96de6db31cb34810bab503906d37ffa3"
      ],
      "layout": "IPY_MODEL_95b19c1008d9476d90ce2428ec830cbd"
     }
    },
    "28c1d8c47b0d41a7aea4e4eeb711daa7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "28ea636469a74af2916baa976379c761": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_731caaf300ba4da8a02da6951596fd99",
      "placeholder": "​",
      "style": "IPY_MODEL_8c0e3e14eee044dc9bbf5fcfe8a6b853",
      "value": " 1.29G/1.29G [00:36&lt;00:00, 24.7MB/s]"
     }
    },
    "3057b9dfcdc54b8d85df164743b2d816": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_42b645c7747e4ab3b14a7132174d62c2",
      "placeholder": "​",
      "style": "IPY_MODEL_2678115b6c0543a48db475aa836ad3d0",
      "value": "Downloading: 100%"
     }
    },
    "31c848ab8292448bb6b7f3689a01ae42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "342b2d59758743d18e8b7be77f47d2cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9951679739c94f58bcad9a07974371df",
      "placeholder": "​",
      "style": "IPY_MODEL_181442a9ff0547d28f4aa33423b8c1b2",
      "value": "Downloading data: 100%"
     }
    },
    "365d539c8d904e84869d87cfe4826776": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38922a79fbe2414589d76f73a79a0635": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_19fbd956215a4b8987f1fab9e1cc4a75",
      "placeholder": "​",
      "style": "IPY_MODEL_d4cc299e543145e49c51c5a5a1194326",
      "value": " 2/2 [00:37&lt;00:00, 18.54s/it]"
     }
    },
    "389b7172c6c34870b38e4cfe7e8267cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d52ab756e4014fbd813cc236e1d80a8e",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e09fe2cefb4449dcbc3f329fe8531682",
      "value": 1
     }
    },
    "3ce4e85230eb4f22bdd1214fb963182b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "41dfba0fc6634e3e87744e16b168fcd4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "42b645c7747e4ab3b14a7132174d62c2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46b29ecc681b4f4bb50d53139eb71ed5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4989973854db404d9778dc255dda8b5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f01aaaf2349243f49ed30165209c226f",
       "IPY_MODEL_725cf25b463e4f96909a9ae4c119ce3a",
       "IPY_MODEL_0c316286f2234849a2ea62780ecb72bc"
      ],
      "layout": "IPY_MODEL_15a85533473a441fb198aa9b6d8f1109"
     }
    },
    "4ae819c352f1436e8a8edbf070b614a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4cefa04beb45437b82585dce4f2dc07a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4d6a202047344cf68643ead0dd62125c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_580d26863a6045ba88e5faa303426551",
      "placeholder": "​",
      "style": "IPY_MODEL_04ae4ba4bb3e4f1c9d181303a74dd901",
      "value": " 4674/0 [00:21&lt;00:00, 230.60 examples/s]"
     }
    },
    "4e204ed303614cae8900afe9ac6a8ff9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3057b9dfcdc54b8d85df164743b2d816",
       "IPY_MODEL_60d57f563fc34d43a7f247535591915c",
       "IPY_MODEL_cc6c5c0f01d4488e893ed17549a5c352"
      ],
      "layout": "IPY_MODEL_ca33183589f24d3eb88e8a8768bd3793"
     }
    },
    "4f16305129c845f7a52c40be80b8998b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4f5119ceba7d4893a216c04727c162d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "517071c32d1540f987393c62c9ae7fda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "569b94e34cab4a59acfb5cc18c82a4d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56f30b99e3a2416b8f4aa25422d846b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_efd59de2ec8f486eab54d39b14835b27",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3ce4e85230eb4f22bdd1214fb963182b",
      "value": 2
     }
    },
    "5779574a9dbe4a309fffa69d2a0fecea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0cff02e713b7475f90e86a511a13e27b",
      "placeholder": "​",
      "style": "IPY_MODEL_070ed550a6ba4a7aaaeb3ba15c4066a0",
      "value": "Downloading data files: 100%"
     }
    },
    "580d26863a6045ba88e5faa303426551": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5976191eec824afd8a28394e3b54a2fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5c7454324a0f435cb02740e1a7c329a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd00f5d4b930408fa6906459c550c4a9",
      "placeholder": "​",
      "style": "IPY_MODEL_7e6de5e403534d0097ab835d4efec819",
      "value": " 12.5k/12.5k [00:00&lt;00:00, 214kB/s]"
     }
    },
    "60ca3e674bb5446e90fb8800190ceab0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c9dfe42049d0417699ad332110998f38",
      "placeholder": "​",
      "style": "IPY_MODEL_7adae587a4f542adacca30641df7ffb8",
      "value": "Downloading: 100%"
     }
    },
    "60d57f563fc34d43a7f247535591915c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_92aa058fc276497182afd25da53586ee",
      "max": 294364,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_09d7f2349f034892999122bc3f163d39",
      "value": 294364
     }
    },
    "62eba812c1ea420bbba424761ae04512": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6350069db64b4de9a20bfba57cb307b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63f21c09f0c142b78881d58de21da94e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "658b1810ed534be696ce167905293691": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "67374604e7264f6abdf54c8edc062842": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "689fe2225f614aef9fd7e3786fc63d85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0eff06df5d6641ee8ac379e31ac59fda",
      "placeholder": "​",
      "style": "IPY_MODEL_4f5119ceba7d4893a216c04727c162d7",
      "value": " 2/2 [00:00&lt;00:00, 37.34it/s]"
     }
    },
    "6b90aaef56614dca8337d0049908ccaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d8c0a29c8e444d5f83a2d2db6a00f70e",
       "IPY_MODEL_c0c5f585eed542fd86989f87d3230f87",
       "IPY_MODEL_7ac9b67cd2e74ec680d3682e26c7d62e"
      ],
      "layout": "IPY_MODEL_f432e1323022494682c1a642d5ae97d4"
     }
    },
    "6e5962ddd1ad4fe58cafb364e02f05ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "725cf25b463e4f96909a9ae4c119ce3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b29352f173934596a549082fc978a9b7",
      "max": 1568,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_02dbb05070de4fb99038f3c1459d971b",
      "value": 1568
     }
    },
    "72e6a1befe094dcca90ba6a331b2fb7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1beab5cdb547417aaa9f000456318095",
       "IPY_MODEL_8f20595f01ae43cab4267e6464aa9772",
       "IPY_MODEL_c1c0ef5a62a24f9cb0856c333d45f9c0"
      ],
      "layout": "IPY_MODEL_95ef1c79b7734a65826842bd8771bf30"
     }
    },
    "731caaf300ba4da8a02da6951596fd99": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75516b3bbcd64743ba028bab31576813": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_75c2dbc8dbb54620b1008e43f28d8855",
      "placeholder": "​",
      "style": "IPY_MODEL_d78b1cedb1524233b8c803e1bb8b64f9",
      "value": "Extracting data files: 100%"
     }
    },
    "75c2dbc8dbb54620b1008e43f28d8855": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a01aa7f65cb41eb82f1082484873bec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "7ac9b67cd2e74ec680d3682e26c7d62e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d3c49e2dfa2f4b6db7f267758e90ed18",
      "placeholder": "​",
      "style": "IPY_MODEL_0f3887f816b94e22bb8a6af06ec4c0af",
      "value": " 892M/892M [00:41&lt;00:00, 48.4MB/s]"
     }
    },
    "7adae587a4f542adacca30641df7ffb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7e6de5e403534d0097ab835d4efec819": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7e78c450df2547dbba98e905bf9e51fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ec0d2e7beeb453ca105e937e25e4649": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_21068c7def7f430aa7dde408bb4bbcf8",
       "IPY_MODEL_db77ee59a9a04054b905ad6029194f40",
       "IPY_MODEL_5c7454324a0f435cb02740e1a7c329a1"
      ],
      "layout": "IPY_MODEL_e765fb6b14a841ffae367ea9a1c10673"
     }
    },
    "7ecce60e38414b5e816f0a73d348980d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5779574a9dbe4a309fffa69d2a0fecea",
       "IPY_MODEL_56f30b99e3a2416b8f4aa25422d846b2",
       "IPY_MODEL_38922a79fbe2414589d76f73a79a0635"
      ],
      "layout": "IPY_MODEL_7f370a102aa94d3e9f518910fe73deac"
     }
    },
    "7f370a102aa94d3e9f518910fe73deac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f37259c65c24896aee34c6ef91b56d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "840ef064a4834daaa8a36ef53b0d6fff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87108eee975446f6843700cb9c1e7260": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_569b94e34cab4a59acfb5cc18c82a4d5",
      "placeholder": "​",
      "style": "IPY_MODEL_5976191eec824afd8a28394e3b54a2fa",
      "value": "Generating test split: "
     }
    },
    "8bb57d4a1fca4304a0880f59e304f957": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a4bd17fcb67d4504a520b5ab89a3cf7a",
       "IPY_MODEL_df4030a26afa48cf98cea1eef8c1719e",
       "IPY_MODEL_0dc81039a8a44c92b01c620b318661f4"
      ],
      "layout": "IPY_MODEL_8ca42afeb33c48d88d4fe6e91b02a453"
     }
    },
    "8c0e3e14eee044dc9bbf5fcfe8a6b853": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8ca42afeb33c48d88d4fe6e91b02a453": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d88b9208d4a4cf9bee8328b634ae9dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f20595f01ae43cab4267e6464aa9772": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a01aa7f65cb41eb82f1082484873bec",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_17aeb3f915f14cbb80287500a1962a29",
      "value": 1
     }
    },
    "8ff46e04f6ab4ed2869bc3895269d9a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "91edd4419ffa435cab242d6f6a1bd4ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92aa058fc276497182afd25da53586ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95b19c1008d9476d90ce2428ec830cbd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95ef1c79b7734a65826842bd8771bf30": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "969ea53b2bbe4e9db098c413dae1a0e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_75516b3bbcd64743ba028bab31576813",
       "IPY_MODEL_ab76d1693f9641079c0d0ee19b7aa096",
       "IPY_MODEL_689fe2225f614aef9fd7e3786fc63d85"
      ],
      "layout": "IPY_MODEL_67374604e7264f6abdf54c8edc062842"
     }
    },
    "96de6db31cb34810bab503906d37ffa3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_91edd4419ffa435cab242d6f6a1bd4ae",
      "placeholder": "​",
      "style": "IPY_MODEL_7f37259c65c24896aee34c6ef91b56d2",
      "value": " 2.00/2.00 [00:00&lt;00:00, 42.9B/s]"
     }
    },
    "9951679739c94f58bcad9a07974371df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1d0d228170247e2a62c1dd490114481": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a4bd17fcb67d4504a520b5ab89a3cf7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28c1d8c47b0d41a7aea4e4eeb711daa7",
      "placeholder": "​",
      "style": "IPY_MODEL_ce50f086f2c74661a44589422022b38d",
      "value": "Downloading: 100%"
     }
    },
    "a78807ba67b34e2b8f306e49603e895b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f350d756c0354010b4f5d45ec779d9cc",
      "max": 1292436853,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ec70235fc1d74a2499dad4db00296ef8",
      "value": 1292436853
     }
    },
    "a7c3e7b8ade14c41b77f9d3bd357dddb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab76d1693f9641079c0d0ee19b7aa096": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b220d2c5753349a39eaca4e2f341fd6b",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_31c848ab8292448bb6b7f3689a01ae42",
      "value": 2
     }
    },
    "b13708015b48461d904803cb3ddaf898": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b20913079b8d4e6f8acc14a97e4c1302": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b220d2c5753349a39eaca4e2f341fd6b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b29352f173934596a549082fc978a9b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b3fd6337545a4f40b38a6c0002071576": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_87108eee975446f6843700cb9c1e7260",
       "IPY_MODEL_389b7172c6c34870b38e4cfe7e8267cd",
       "IPY_MODEL_4d6a202047344cf68643ead0dd62125c"
      ],
      "layout": "IPY_MODEL_e6eb0bcfba554f0face54edda32bf55d"
     }
    },
    "b5d96b10092e4377b95d072e56f16ac5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bdceb0ea847a46ad8e2a4eb0bff2ff47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0c5f585eed542fd86989f87d3230f87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b13708015b48461d904803cb3ddaf898",
      "max": 891641279,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_46b29ecc681b4f4bb50d53139eb71ed5",
      "value": 891641279
     }
    },
    "c1c0ef5a62a24f9cb0856c333d45f9c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d88b9208d4a4cf9bee8328b634ae9dd",
      "placeholder": "​",
      "style": "IPY_MODEL_4f16305129c845f7a52c40be80b8998b",
      "value": " 4536/0 [00:01&lt;00:00, 3733.42 examples/s]"
     }
    },
    "c9dfe42049d0417699ad332110998f38": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca33183589f24d3eb88e8a8768bd3793": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc6c5c0f01d4488e893ed17549a5c352": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bdceb0ea847a46ad8e2a4eb0bff2ff47",
      "placeholder": "​",
      "style": "IPY_MODEL_02b89793ece04cb882adc21c6b5017df",
      "value": " 294k/294k [00:00&lt;00:00, 1.39MB/s]"
     }
    },
    "ce50f086f2c74661a44589422022b38d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d13e0fd435b74c3cbcacf8d7518a3c05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d3c49e2dfa2f4b6db7f267758e90ed18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4cc299e543145e49c51c5a5a1194326": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d4d35361775c4f08af96044d6641e3cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6350069db64b4de9a20bfba57cb307b7",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_41dfba0fc6634e3e87744e16b168fcd4",
      "value": 2
     }
    },
    "d52ab756e4014fbd813cc236e1d80a8e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "d78b1cedb1524233b8c803e1bb8b64f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d8b0cab1a00c4ada8a354355b94028b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d8c0a29c8e444d5f83a2d2db6a00f70e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1499dca31374425e90ca96e0a8f0eb4c",
      "placeholder": "​",
      "style": "IPY_MODEL_d8b0cab1a00c4ada8a354355b94028b1",
      "value": "Downloading: 100%"
     }
    },
    "db77ee59a9a04054b905ad6029194f40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eded64c3dfae4bc39806e54dcdaecb1d",
      "max": 12512,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_277ebcdd32df4b8db9085d9594e2fae5",
      "value": 12512
     }
    },
    "df4030a26afa48cf98cea1eef8c1719e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_63f21c09f0c142b78881d58de21da94e",
      "max": 703051,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6e5962ddd1ad4fe58cafb364e02f05ea",
      "value": 703051
     }
    },
    "e09fe2cefb4449dcbc3f329fe8531682": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e1715240dd784cc684cbf7c6839e43d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6eb0bcfba554f0face54edda32bf55d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "e765fb6b14a841ffae367ea9a1c10673": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec70235fc1d74a2499dad4db00296ef8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eded64c3dfae4bc39806e54dcdaecb1d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "efd59de2ec8f486eab54d39b14835b27": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f01aaaf2349243f49ed30165209c226f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7e78c450df2547dbba98e905bf9e51fe",
      "placeholder": "​",
      "style": "IPY_MODEL_d13e0fd435b74c3cbcacf8d7518a3c05",
      "value": "Downloading: 100%"
     }
    },
    "f350d756c0354010b4f5d45ec779d9cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f432e1323022494682c1a642d5ae97d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9619fca2b574be281e94028d9be8fc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_60ca3e674bb5446e90fb8800190ceab0",
       "IPY_MODEL_ff3a1a0b7cea449f9843b998a56ec3b2",
       "IPY_MODEL_12535537221449a08b56ffad4da68b74"
      ],
      "layout": "IPY_MODEL_e1715240dd784cc684cbf7c6839e43d8"
     }
    },
    "fd00f5d4b930408fa6906459c550c4a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff3a1a0b7cea449f9843b998a56ec3b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_658b1810ed534be696ce167905293691",
      "max": 1477,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a1d0d228170247e2a62c1dd490114481",
      "value": 1477
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
